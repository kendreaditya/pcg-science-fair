
@article{bau_gan_2018,
	title = {{GAN} {Dissection}: {Visualizing} and {Understanding} {Generative} {Adversarial} {Networks}},
	shorttitle = {{GAN} {Dissection}},
	url = {http://arxiv.org/abs/1811.10597},
	abstract = {Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.},
	urldate = {2021-01-10},
	journal = {arXiv:1811.10597 [cs]},
	author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B. and Freeman, William T. and Torralba, Antonio},
	month = dec,
	year = {2018},
	note = {arXiv: 1811.10597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {Bau et al_2018_GAN Dissection.pdf:C\:\\Users\\kendr\\Zotero\\storage\\UFBZFCW7\\Bau et al_2018_GAN Dissection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\H9DTYGGY\\1811.html:text/html},
}

@book{d_detection_nodate,
	title = {Detection of {Heart} {Diseases} by {Mathematical} {Artificial} {Intelligence} {Algorithm} {Using} {Phonocardiogram} {Signals}},
	abstract = {Copyright © 2013 ISSR Journals. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT: An artificial intelligence (AI) algorithm has been developed using Mathematical formula to diagnose heart disease from Phonocardiogram (PCG) signals. Auscultation, the technique of listening to heart sounds with a stethoscope can be used as a primary detection technique for detecting heart disorders for the past years. But now the Phonocardiogram, the digital recording of heart sounds is becoming very popular technique as it is relatively inexpensive. Four amplitude parameters of the PCG signal are extracted by using filter technique and are used as input. PCG signals for three types of heart diseases such as Tachycardia, Bradycardia and Atrial fibrillation were used in this paper to test the accuracy. These disease types that affect the electrical system of heart are known as arrhythmias, cause the heart to beat very fast (Tachycardia) or very slow (Bradycardia), or unexpectedly (Atrial fibrillation). After the signals are filtered and the parameters are extracted, the parameters are fed to the AI algorithm. Classifications of heart diseases are carried using the AI algorithm by comparing the extracted parameters. Here comparison is done using Min Max method. The developed mathematical artificial intelligence algorithm is implemented in MATLab using Simulink and the simulation results proved that the developed algorithm has been shown to be a powerful technique in detection of heart diseases using PCG signals.},
	author = {D, Prakash and T, Uma Mageshwari and K, Prabakaran and A, Suguna},
	file = {D et al_Detection of Heart Diseases by Mathematical Artificial Intelligence Algorithm.pdf:C\:\\Users\\kendr\\Zotero\\storage\\U5NRTT5V\\D et al_Detection of Heart Diseases by Mathematical Artificial Intelligence Algorithm.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\IAB52DHP\\download.html:text/html},
}

@misc{noauthor_physionetcinc_nodate,
	title = {{PhysioNet}/{CinC} {Challenge} 2016: {Training} {Sets}},
	url = {https://archive.physionet.org/pn3/challenge/2016/},
	urldate = {2021-01-11},
	file = {PhysioNet/CinC Challenge 2016\: Training Sets:C\:\\Users\\kendr\\Zotero\\storage\\KSU4Z6PL\\2016.html:text/html},
}

@article{rawther_detection_2015,
	title = {Detection and {Classification} of {Cardiac} {Arrhythmias} based on {ECG} and {PCG} using {Temporal} and {Wavelet} features},
	volume = {4},
	abstract = {Arrhythmias are abnormal rhythms of heart. Sudden Cardiac Arrest is most often caused by life threatening arrhythmias such as Ventricular Tachycardia (VT) and Ventricular Fibrillation (VF). Early detection of life threatening arrhythmias is crucial for successive defibrillation therapy. In general heart diseases have been investigated by various methods. Among these Electrocardiography (ECG) test is considered as the best noninvasive method of investigation. ECG test is varied if the heart sound from the Phonocardiogram shows any abnormalities. So Phonocardiography (PCG) is also considered for more efficiency. Commonly used arrhythmia detection and classification algorithms are only based on surface Electrocardiogram analysis. So an algorithm corresponds to multiresolution wavelet analysis using temporal and wavelet features of Electrocardiogram and Phonocardiogram along with Electrocardiogram-Phonocardiogram relationships is designed so as to increase the efficiency of the heart diagnostics. Temporal and wavelet features of ECG and PCG along with the linear prediction coefficients representing ECG and PCG are fed to the classifier for classification. The goal of this work is to achieve an efficient arrhythmia detection system that can lead to high performance heart diagnostics.},
	language = {en},
	number = {4},
	author = {Rawther, Nabina N and Cheriyan, Jini},
	year = {2015},
	pages = {6},
	file = {Rawther and Cheriyan - 2015 - Detection and Classification of Cardiac Arrhythmia.pdf:C\:\\Users\\kendr\\Zotero\\storage\\Q2B7GTV6\\Rawther and Cheriyan - 2015 - Detection and Classification of Cardiac Arrhythmia.pdf:application/pdf},
}

@article{ismail_localization_2018,
	title = {Localization and classification of heart beats in phonocardiography signals —a comprehensive review},
	volume = {2018},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-018-0545-9},
	doi = {10.1186/s13634-018-0545-9},
	abstract = {Phonocardiogram (PCG) signal represents recording of sounds and murmurs resulting from heart auscultation. Analysis of these PCG signals is critical in diagnosis of different heart diseases. Over the years, a variety of methods have been proposed for automatic analysis of PCG signals in time, frequency, and time-frequency domains. This paper presents a comprehensive survey of different methods proposed for automatic analysis of PCG signals with the objective to evaluate the current state-of-the-art and to determine the potential domains of effective analysis. An important aspect of our contribution is that the review is carried out as a function of domains of analysis rather than simply discussing different methods. Our method further splits analysis into pre-processing, localization, and classification, and details are presented in terms of techniques and classifiers used during these phases. Finally, results are summarized for normal heart beat, noisy heart beat, and different pathologies using metrices like accuracy and detection rate. In addition to time and frequency domain, time-frequency based methods including wavelet, empirical mode decomposition (EMD) and time-frequency representation (TFR) are selected for detailed analysis. The review concludes that the time-frequency representations like EMD and wavelets represent areas of exploration in future along with perspective of using different time-frequency techniques together.},
	language = {en},
	number = {1},
	urldate = {2021-01-12},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Ismail, Shahid and Siddiqi, Imran and Akram, Usman},
	month = dec,
	year = {2018},
	pages = {26},
	file = {Ismail et al. - 2018 - Localization and classification of heart beats in .pdf:C\:\\Users\\kendr\\Zotero\\storage\\RSE7C8IF\\Ismail et al. - 2018 - Localization and classification of heart beats in .pdf:application/pdf},
}

@article{deperlioglu_diagnosis_2020,
	title = {Diagnosis of heart diseases by a secure {Internet} of {Health} {Things} system based on {Autoencoder} {Deep} {Neural} {Network}},
	volume = {162},
	issn = {01403664},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140366420318909},
	doi = {10.1016/j.comcom.2020.08.011},
	abstract = {Objective of this study is to introduce a secure IoHT system, which acts as a clinical decision support system with the diagnosis of cardiovascular diseases. In this sense, it was emphasized that the accuracy rate of diagnosis (classification) can be improved via deep learning algorithms, by needing no hybrid-complex models, and a secure data processing can be achieved with a multi-authentication and Tangle based approach. In detail, heart sounds were classified with Autoencoder Neural Networks (AEN) and the IoHT system was built for supporting doctors in real-time. For developing the diagnosis infrastructure by the AEN, PASCAL B-Training and Physiobank-PhysioNet A-Training heart sound datasets were used accordingly. For the PASCAL dataset, the AEN provided a diagnosis-classification performance with the accuracy of 100\%, sensitivity of 100\%, and the specificity of 100\% whereas the rates were respectively 99.8\%, 99.65\%, and 99.13\% for the PhysioNet dataset. It was seen that the findings by the developed AEN based solution were better than the alternative solutions from the literature. Additionally, usability of the whole IoHT system was found positive by the doctors, and according to the 479 real-case applications, the system was able to achieve accuracy rates of 96.03\% for normal heart sounds, 91.91\% for extrasystole, and 90.11\% for murmur. In terms of security approach, the system was also robust against several attacking methods including synthetic data impute as well as trying to penetrating to the system via central system or mobile devices.},
	language = {en},
	urldate = {2021-01-12},
	journal = {Computer Communications},
	author = {Deperlioglu, Omer and Kose, Utku and Gupta, Deepak and Khanna, Ashish and Sangaiah, Arun Kumar},
	month = oct,
	year = {2020},
	pages = {31--50},
	file = {Deperlioglu et al. - 2020 - Diagnosis of heart diseases by a secure Internet o.pdf:C\:\\Users\\kendr\\Zotero\\storage\\7GJS6QXK\\Deperlioglu et al. - 2020 - Diagnosis of heart diseases by a secure Internet o.pdf:application/pdf},
}

@article{surukutla_cardiac_2020,
	title = {Cardiac {Arrhythmia} {Detection} {Using} {CNN}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3572237},
	doi = {10.2139/ssrn.3572237},
	abstract = {The diagnosis of cardiac arrhythmias can be a tedious process when done by hand and could benefit greatly from computer automation. To this end, an algorithm was developed to distinguish between normal heart beats and abnormal arrhythmic beats in an PCG Recording. First an algorithm was developed to find the location of QRS complexes in the PCG Recording. Principal component analysis was performed using the area around the QRS complex. 20 of the resulting principal components were used to train a simple linear classifier to distinguish between normal and abnormal beats. The classification performed reasonably well with a sensitivity of 85.4\% and specificity of 91.7\%. Moresophisticated signal processing and classification techniques could be applied to improve these numbers, but the algorithm is a good starting point.},
	language = {en},
	urldate = {2021-01-12},
	journal = {SSRN Electronic Journal},
	author = {Surukutla, Dinesh and Bhanushali, Karan and Patil, Trupti},
	year = {2020},
	file = {Surukutla et al. - 2020 - Cardiac Arrhythmia Detection Using CNN.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SSWCKM2S\\Surukutla et al. - 2020 - Cardiac Arrhythmia Detection Using CNN.pdf:application/pdf},
}

@incollection{subasi_biomedical_2019,
	title = {Biomedical {Signals}},
	isbn = {978-0-12-817444-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128174449000027},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {Practical {Guide} for {Biomedical} {Signals} {Analysis} {Using} {Machine} {Learning} {Techniques}},
	publisher = {Elsevier},
	author = {Subasi, Abdulhamit},
	year = {2019},
	doi = {10.1016/B978-0-12-817444-9.00002-7},
	pages = {27--87},
	file = {Subasi - 2019 - Biomedical Signals.pdf:C\:\\Users\\kendr\\Zotero\\storage\\ACZ2RXSR\\Subasi - 2019 - Biomedical Signals.pdf:application/pdf},
}

@inproceedings{zhou_learning_2016,
	address = {Las Vegas, NV, USA},
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780688/},
	doi = {10.1109/CVPR.2016.319},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we ﬁnd that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classiﬁcation task1.},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	month = jun,
	year = {2016},
	pages = {2921--2929},
	file = {Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf:C\:\\Users\\kendr\\Zotero\\storage\\X6A45GK6\\Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf:application/pdf},
}

@article{krishnan_automated_2020,
	title = {Automated heart sound classification system from unsegmented phonocardiogram ({PCG}) using deep neural network},
	volume = {43},
	issn = {2662-4729, 2662-4737},
	url = {http://link.springer.com/10.1007/s13246-020-00851-w},
	doi = {10.1007/s13246-020-00851-w},
	abstract = {Given the patient to doctor ratio of 50,000:1 in low income and middle-income countries, there is a need for automated heart sound classification system that can screen the Phonocardiogram (PCG) records in real-time. This paper proposes deep neural network architectures such as a one-dimensional convolutional neural network (1D-CNN) and Feed-forward Neural Network (F-NN) for the classification of unsegmented phonocardiogram (PCG) signal. The research paper aims to automate the feature engineering and feature selection process used in the analysis of the PCG signal. The original PCG signal is down-sampled at 500 Hz. Then they are divided into smaller time segments of 6 s epochs. Savitzky–Golay filter is used to suppress the high-frequency noises in the signal by data point smoothening. The processed data was then provided as an input to the proposed deep neural network (DNN) architectures. 1081 PCG records were used for training and validating the proposed DNN models. The Feed-forward Neural Network model with five hidden layers provided a better overall accuracy of 0.8565 with a sensitivity of 0.8673, and specificity of 0.8475. The balanced accuracy of the model was found to be 0.8574. The performance of the model was also studied using the Receiver Operating Characteristic (ROC) plot, which produced an Area Under the Curve (AUC) value of 0.857. The classification accuracy of the proposed models was compared to the related works on PCG signal analysis for cardiovascular disease detection. The DNN models studied in this study provided comparable performance in heart sound classification without the requirement of feature engineering and segmentation of heart sound signals.},
	language = {en},
	number = {2},
	urldate = {2021-01-12},
	journal = {Physical and Engineering Sciences in Medicine},
	author = {Krishnan, Palani Thanaraj and Balasubramanian, Parvathavarthini and Umapathy, Snekhalatha},
	month = jun,
	year = {2020},
	pages = {505--515},
	file = {Krishnan et al. - 2020 - Automated heart sound classification system from u.pdf:C\:\\Users\\kendr\\Zotero\\storage\\UJ2U4CM9\\Krishnan et al. - 2020 - Automated heart sound classification system from u.pdf:application/pdf},
}

@article{das_human_2016,
	title = {Human {Attention} in {Visual} {Question} {Answering}: {Do} {Humans} and {Deep} {Networks} {Look} at the {Same} {Regions}?},
	shorttitle = {Human {Attention} in {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1606.03556},
	abstract = {We conduct large-scale studies on ‘human attention’ in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1606.03556 [cs]},
	author = {Das, Abhishek and Agrawal, Harsh and Zitnick, C. Lawrence and Parikh, Devi and Batra, Dhruv},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Das et al. - 2016 - Human Attention in Visual Question Answering Do H.pdf:C\:\\Users\\kendr\\Zotero\\storage\\DD59S5CC\\Das et al. - 2016 - Human Attention in Visual Question Answering Do H.pdf:application/pdf},
}

@article{rajpurkar_cardiologist-level_2017,
	title = {Cardiologist-{Level} {Arrhythmia} {Detection} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1707.01836},
	abstract = {We develop an algorithm which exceeds the performance of board certiﬁed cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of ECG samples to a sequence of rhythm classes. Committees of boardcertiﬁed cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value).},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1707.01836 [cs]},
	author = {Rajpurkar, Pranav and Hannun, Awni Y. and Haghpanahi, Masoumeh and Bourn, Codie and Ng, Andrew Y.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01836},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Rajpurkar et al. - 2017 - Cardiologist-Level Arrhythmia Detection with Convo.pdf:C\:\\Users\\kendr\\Zotero\\storage\\YHQTNL4C\\Rajpurkar et al. - 2017 - Cardiologist-Level Arrhythmia Detection with Convo.pdf:application/pdf},
}

@article{fernando_heart_2020,
	title = {Heart {Sound} {Segmentation} using {Bidirectional} {LSTMs} with {Attention}},
	volume = {24},
	issn = {2168-2194, 2168-2208},
	url = {http://arxiv.org/abs/2004.03712},
	doi = {10.1109/JBHI.2019.2949516},
	abstract = {Objective: This paper proposes a novel framework for the segmentation of phonocardiogram (PCG) signals into heart states, exploiting the temporal evolution of the PCG as well as considering the salient information that it provides for the detection of the heart state. Methods: We propose the use of recurrent neural networks and exploit recent advancements in attention based learning to segment the PCG signal. This allows the network to identify the most salient aspects of the signal and disregard uninformative information. Results: The proposed method attains state-of-the-art performance on multiple benchmarks including both human and animal heart recordings. Furthermore, we empirically analyse different feature combinations including envelop features, wavelet and Mel Frequency Cepstral Coefﬁcients (MFCC), and provide quantitative measurements that explore the importance of different features in the proposed approach. Conclusion: We demonstrate that a recurrent neural network coupled with attention mechanisms can effectively learn from irregular and noisy PCG recordings. Our analysis of different feature combinations shows that MFCC features and their derivatives offer the best performance compared to classical wavelet and envelop features. Signiﬁcance: Heart sound segmentation is a crucial pre-processing step for many diagnostic applications. The proposed method provides a cost effective alternative to labour extensive manual segmentation, and provides a more accurate segmentation than existing methods. As such, it can improve the performance of further analysis including the detection of murmurs and ejection clicks. The proposed method is also applicable for detection and segmentation of other one dimensional biomedical signals.},
	language = {en},
	number = {6},
	urldate = {2021-01-12},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Fernando, Tharindu and Ghaemmaghami, Houman and Denman, Simon and Sridharan, Sridha and Hussain, Nayyar and Fookes, Clinton},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.03712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, Quantitative Biology - Quantitative Methods},
	pages = {1601--1609},
	file = {Fernando et al. - 2020 - Heart Sound Segmentation using Bidirectional LSTMs.pdf:C\:\\Users\\kendr\\Zotero\\storage\\QNTW2APB\\Fernando et al. - 2020 - Heart Sound Segmentation using Bidirectional LSTMs.pdf:application/pdf},
}

@inproceedings{almasi_dynamical_2011,
	address = {Boston, MA},
	title = {A dynamical model for generating synthetic {Phonocardiogram} signals},
	isbn = {978-1-4577-1589-1 978-1-4244-4121-1 978-1-4244-4122-8},
	url = {http://ieeexplore.ieee.org/document/6091376/},
	doi = {10.1109/IEMBS.2011.6091376},
	abstract = {In this paper we introduce a dynamical model for Phonocardiogram (PCG) signal which is capable of generating realistic synthetic PCG signals. This model is based on PCG morphology and consists of three ordinary differential equations and can represent various morphologies of normal PCG signals. Beat-to-beat variation in PCG morphology is significant so model parameters vary from beat to beat. This model is inspired of Electrocardiogram (ECG) dynamical model proposed by McSharry et al. and can be employed to assess biomedical signal processing techniques.},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {2011 {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}},
	publisher = {IEEE},
	author = {Almasi, A. and Shamsollahi, M. B. and Senhadji, L.},
	month = aug,
	year = {2011},
	pages = {5686--5689},
	file = {Almasi et al. - 2011 - A dynamical model for generating synthetic Phonoca.pdf:C\:\\Users\\kendr\\Zotero\\storage\\RYXT4H8E\\Almasi et al. - 2011 - A dynamical model for generating synthetic Phonoca.pdf:application/pdf},
}

@article{kalaivani_diagnosis_2014,
	title = {{DIAGNOSIS} {OF} {ARRHYTHMIA} {DISEASES} {USING} {HEART} {SOUNDS} {AND} {ECG} {SIGNALS}},
	issn = {1560-4071},
	url = {http://russjcardiol.elpub.ru/jour/article/view/590},
	doi = {10.15829/1560-4071-2014-1-ENG-35-41},
	language = {en},
	number = {1-ENG},
	urldate = {2021-01-12},
	journal = {Russian Journal of Cardiology},
	author = {Kalaivani, V.},
	month = jan,
	year = {2014},
	pages = {35--41},
	file = {Kalaivani - 2014 - DIAGNOSIS OF ARRHYTHMIA DISEASES USING HEART SOUND.pdf:C\:\\Users\\kendr\\Zotero\\storage\\IY5WHD2L\\Kalaivani - 2014 - DIAGNOSIS OF ARRHYTHMIA DISEASES USING HEART SOUND.pdf:application/pdf},
}

@article{olah_feature_2017,
	title = {Feature {Visualization}},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	language = {en},
	number = {11},
	urldate = {2021-01-12},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	month = nov,
	year = {2017},
	pages = {10.23915/distill.00007},
	file = {Olah et al. - 2017 - Feature Visualization.pdf:C\:\\Users\\kendr\\Zotero\\storage\\M8BE9DVH\\Olah et al. - 2017 - Feature Visualization.pdf:application/pdf},
}

@article{finlay_mobile-phonocardiogram_2006,
	title = {The "mobile-phonocardiogram", a new tool in the arrhythmia clinic},
	volume = {92},
	issn = {1355-6037},
	url = {https://heart.bmj.com/lookup/doi/10.1136/hrt.2005.076315},
	doi = {10.1136/hrt.2005.076315},
	language = {en},
	number = {7},
	urldate = {2021-01-12},
	journal = {Heart},
	author = {Finlay, M},
	month = may,
	year = {2006},
	pages = {898--898},
	file = {Finlay - 2006 - The mobile-phonocardiogram, a new tool in the ar.pdf:C\:\\Users\\kendr\\Zotero\\storage\\EBZ3J29W\\Finlay - 2006 - The mobile-phonocardiogram, a new tool in the ar.pdf:application/pdf},
}

@inproceedings{garg_heart_2019,
	address = {Noida, India},
	title = {Heart {Rhythm} {Abnormality} {Detection} from {PCG} {Signal}},
	isbn = {978-1-72813-591-5},
	url = {https://ieeexplore.ieee.org/document/8844950/},
	doi = {10.1109/IC3.2019.8844950},
	abstract = {Cardiovascular diseases are the major cause of mortality and PCG provides a non-invasive way to monitor the heart. In this paper, we give a unique approach to classify Normal and Abnormal heart rhythms using Machine Learning. The heart sounds are digital signals recorded from electronic stethoscope. In the initial phase Signal Quality assessment and feature extraction is done after which we explore different data models to find the relation between the features and the results, achieving poor results. In the second phase, audio files are segmented into Systolic and Diastolic phases using a Logistic Regression-HSMM These segments of systoles and diastoles are then individually analyzed and individual feature extraction is done. In the segmentation process a lot of de-noising is also done removing the background noises. This approach yields an accuracy of 79\% which concludes that analyzing the heart signal at Systolic and Diastolic phase is a very essential step to solve this problem.},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {2019 {Twelfth} {International} {Conference} on {Contemporary} {Computing} ({IC3})},
	publisher = {IEEE},
	author = {Garg, Varsha and Mathur, Arpit and Mangla, Nishant and Rawat, Aman Singh},
	month = aug,
	year = {2019},
	pages = {1--5},
	file = {Garg et al. - 2019 - Heart Rhythm Abnormality Detection from PCG Signal.pdf:C\:\\Users\\kendr\\Zotero\\storage\\MLDBMTQR\\Garg et al. - 2019 - Heart Rhythm Abnormality Detection from PCG Signal.pdf:application/pdf},
}

@article{jordaens_clinical_2018,
	title = {A clinical approach to arrhythmias revisited in 2018: {From} {ECG} over noninvasive and invasive electrophysiology to advanced imaging},
	volume = {26},
	issn = {1568-5888, 1876-6250},
	shorttitle = {A clinical approach to arrhythmias revisited in 2018},
	url = {http://link.springer.com/10.1007/s12471-018-1089-1},
	doi = {10.1007/s12471-018-1089-1},
	abstract = {Understanding arrhythmias and their treatment is not always easy. The current straightforward approach with catheter ablation and device therapy is an amazing achievement, but does not make management of underlying or other cardiac disease and pharmacological therapy unnecessary. The goal of this paper is to describe how much of the knowledge of the 1980s and early 1990s can and should still be applied in the modern treatment of patients with arrhythmias. After an introduction, this review will focus on paroxysmal atrial ﬁbrillation and a prototype of ‘idiopathic’ ventricular arrhythmias, two diseases with a striking similarity, and will discuss the arrhythmogenesis. The ECG continues to play an important role in diagnostics. Both diseases are associated with a structurally normal heart; the autonomic nervous system plays an important role in triggering arrhythmias at both the atrial and ventricular level.},
	language = {en},
	number = {4},
	urldate = {2021-01-12},
	journal = {Netherlands Heart Journal},
	author = {Jordaens, L.},
	month = apr,
	year = {2018},
	pages = {182--189},
	file = {Jordaens - 2018 - A clinical approach to arrhythmias revisited in 20.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SVC7LVTY\\Jordaens - 2018 - A clinical approach to arrhythmias revisited in 20.pdf:application/pdf},
}

@article{kiranyaz_real-time_2020,
	title = {Real-time phonocardiogram anomaly detection by adaptive {1D} {Convolutional} {Neural} {Networks}},
	volume = {411},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220309085},
	doi = {10.1016/j.neucom.2020.05.063},
	abstract = {The heart sound signals (Phonocardiogram – PCG) enable the earliest monitoring to detect a potential cardiovascular pathology and have recently become a crucial tool as a diagnostic test in outpatient monitoring to assess heart hemodynamic status. The need for an automated and accurate anomaly detection method for PCG has thus become imminent. To determine the state-of-the-art PCG classiﬁcation algorithm, 48 international teams competed in the PhysioNet (CinC) Challenge in 2016 over the largest benchmark dataset with 3126 records with the classiﬁcation outputs, normal (N), abnormal (A) and unsure – too noisy (U). In this study, our aim is to push this frontier further; however, we focus deliberately on the anomaly detection problem while assuming a reasonably high Signal-to-Noise Ratio (SNR) on the records. By using 1D Convolutional Neural Networks trained with a novel data puriﬁcation approach, we aim to achieve the highest detection performance and real-time processing ability with signiﬁcantly lower delay and computational complexity. The experimental results over the high-quality subset of the same benchmark dataset show that the proposed approach achieves both objectives. Furthermore, our ﬁndings reveal the fact that further improvements indeed require a personalized (patient-speciﬁc) approach to avoid major drawbacks of a global PCG classiﬁcation approach.},
	language = {en},
	urldate = {2021-01-12},
	journal = {Neurocomputing},
	author = {Kiranyaz, Serkan and Zabihi, Morteza and Rad, Ali Bahrami and Ince, Turker and Hamila, Ridha and Gabbouj, Moncef},
	month = oct,
	year = {2020},
	pages = {291--301},
	file = {Kiranyaz et al. - 2020 - Real-time phonocardiogram anomaly detection by ada.pdf:C\:\\Users\\kendr\\Zotero\\storage\\TTYMRV6T\\Kiranyaz et al. - 2020 - Real-time phonocardiogram anomaly detection by ada.pdf:application/pdf},
}

@article{lubaib_heart_2016,
	title = {The {Heart} {Defect} {Analysis} {Based} on {PCG} {Signals} {Using} {Pattern} {Recognition} {Techniques}},
	volume = {24},
	issn = {22120173},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212017316303164},
	doi = {10.1016/j.protcy.2016.05.225},
	abstract = {The graphical recording of the heart sounds and murmurs is called Phonocardiogram or PCG and the machine is so called phonocardiograph. It has an important role in the proper and accurate diagnosis of the heart defects. It requires highly and experienced professionals to read the phonocardiogram, as usually with the stethoscope. The paper is about the implementation of a diagnostic system as a detector and classifier; for heart diseases. Various heart sound samples are classified using Support Vector Machine (SVM), K Nearest Neighbour (KNN), Bayesian and Gaussian Mixture Model (KNN) Classifiers. The output of the system is the classification of the sound as either normal or abnormal and if it is abnormal, what type of abnormality is present. In the proposed method, time domain and frequency domain features are extracted. Various frequency domain features such as energy, mean, variance and Mel Frequency Cepstral Coefficients (MFCC) are analysed.},
	language = {en},
	urldate = {2021-01-12},
	journal = {Procedia Technology},
	author = {Lubaib, P. and Muneer, K.V. Ahammed},
	year = {2016},
	pages = {1024--1031},
	file = {Lubaib and Muneer - 2016 - The Heart Defect Analysis Based on PCG Signals Usi.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SSC4JC38\\Lubaib and Muneer - 2016 - The Heart Defect Analysis Based on PCG Signals Usi.pdf:application/pdf},
}

@article{thoms_real-life_2019,
	title = {Real-life physics: phonocardiography, electrocardiography, and audiometry with a smartphone},
	volume = {1223},
	issn = {1742-6588, 1742-6596},
	shorttitle = {Real-life physics},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1223/1/012007},
	doi = {10.1088/1742-6596/1223/1/012007},
	abstract = {To foster student motivation and engagement, we combined authentic contexts, procedures, and materials by assessing biomedical physics topics with a smartphone. Selected experiments with simple aids allow for the examination of a student’s heartbeat in various ways: e.g., phonocardiography and electrocardiography. In addition, students can test their frequency-dependent hearing threshold. These contexts lead to an understanding of various physics concepts in a meaningful way.},
	language = {en},
	urldate = {2021-01-12},
	journal = {Journal of Physics: Conference Series},
	author = {Thoms, Lars-Jochen and Collichia, Giuseppe and Girwidz, Raimund},
	month = may,
	year = {2019},
	pages = {012007},
	file = {Thoms et al. - 2019 - Real-life physics phonocardiography, electrocardi.pdf:C\:\\Users\\kendr\\Zotero\\storage\\IMBDIDLD\\Thoms et al. - 2019 - Real-life physics phonocardiography, electrocardi.pdf:application/pdf},
}

@article{yupapin_heart_2011,
	title = {Heart detection and diagnosis based on {ECG} and {EPCG} relationships},
	issn = {1179-1470},
	url = {http://www.dovepress.com/heart-detection-and-diagnosis-based-on-ecg-and-epcg-relationships-peer-reviewed-article-MDER},
	doi = {10.2147/MDER.S23324},
	abstract = {A new design of a system for preliminary detection of defective hearts is proposed which is composed of two subsystems, in which one is based on the relationship between the electrocardiogram (ECG) and phonocardiogram (PCG) signals. The relationship between both signals is determined as an impulse response (h(n)) of a system, where the decision is made based on the linear predictive coding coefficients of a heart’s impulse response. The other subsystem uses a phase space approach, in which the mean squared error between the distance vectors of the phase space of the normal heart and abnormal heart is judged by the likelihood ratio test (Λ) value, on which the decision is made. The advantage of the proposed system is that a heart’s diagnosis system based on the ECG and EPCG signals can lead to high performance heart diagnostics.},
	language = {en},
	urldate = {2021-01-12},
	journal = {Medical Devices: Evidence and Research},
	author = {Yupapin, Preecha and {Wardkein} and Yupapin, Preecha and {Phanphaisarn} and {Koseeyaporn} and {Roeksabutr} and {Roeksabutr} and {Wardkein} and {Koseeyapon}},
	month = aug,
	year = {2011},
	pages = {133},
	file = {Yupapin et al. - 2011 - Heart detection and diagnosis based on ECG and EPC.pdf:C\:\\Users\\kendr\\Zotero\\storage\\FMP7KD4P\\Yupapin et al. - 2011 - Heart detection and diagnosis based on ECG and EPC.pdf:application/pdf},
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:C\:\\Users\\kendr\\Zotero\\storage\\XT38YXCF\\Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:C\:\\Users\\kendr\\Zotero\\storage\\JRDWYLE5\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf},
}

@article{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	url = {http://arxiv.org/abs/1706.03825},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classiﬁer, one type of explanation is to identify pixels that strongly inﬂuence the ﬁnal decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:C\:\\Users\\kendr\\Zotero\\storage\\8S3LTZYZ\\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:application/pdf},
}

@article{latif_phonocardiographic_2020,
	title = {Phonocardiographic {Sensing} using {Deep} {Learning} for {Abnormal} {Heartbeat} {Detection}},
	url = {http://arxiv.org/abs/1801.08322},
	abstract = {Cardiac auscultation involves expert interpretation of abnormalities in heart sounds using stethoscope. Deep learning based cardiac auscultation is of signiﬁcant interest to the healthcare community as it can help reducing the burden of manual auscultation with automated detection of abnormal heartbeats. However, the problem of automatic cardiac auscultation is complicated due to the requirement of reliability and high accuracy, and due to the presence of background noise in the heartbeat sound. In this work, we propose a Recurrent Neural Networks (RNNs) based automated cardiac auscultation solution. Our choice of RNNs is motivated by the great success of deep learning in medical applications and by the observation that RNNs represent the deep learning conﬁguration most suitable for dealing with sequential or temporal data even in the presence of noise. We explore the use of various RNN models, and demonstrate that these models deliver the abnormal heartbeat classiﬁcation score with signiﬁcant improvement. Our proposed approach using RNNs can be potentially be used for real-time abnormal heartbeat detection in the Internet of Medical Things for remote monitoring applications.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1801.08322 [cs]},
	author = {Latif, Siddique and Usman, Muhammad and Rana, Rajib and Qadir, Junaid},
	month = jul,
	year = {2020},
	note = {arXiv: 1801.08322},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Latif et al. - 2020 - Phonocardiographic Sensing using Deep Learning for.pdf:C\:\\Users\\kendr\\Zotero\\storage\\LDJZQYZ5\\Latif et al. - 2020 - Phonocardiographic Sensing using Deep Learning for.pdf:application/pdf},
}

@article{ger_autoencoders_2020,
	title = {Autoencoders and {Generative} {Adversarial} {Networks} for {Imbalanced} {Sequence} {Classification}},
	url = {http://arxiv.org/abs/1901.02514},
	abstract = {Generative Adversarial Networks (GANs) have been used in many different applications to generate realistic synthetic data. We introduce a novel GAN with Autoencoder (GAN-AE) architecture to generate synthetic samples for variable length, multi-feature sequence datasets. In this model, we develop a GAN architecture with an additional autoencoder component, where recurrent neural networks (RNNs) are used for each component of the model in order to generate synthetic data to improve classiﬁcation accuracy for a highly imbalanced medical device dataset. In addition to the medical device dataset, we also evaluate the GAN-AE performance on two additional datasets and demonstrate the application of GAN-AE to a sequence-to-sequence task where both synthetic sequence inputs and sequence outputs must be generated. To evaluate the quality of the synthetic data, we train encoder-decoder models both with and without the synthetic data and compare the classiﬁcation model performance. We show that a model trained with GAN-AE generated synthetic data outperforms models trained with synthetic data generated both with standard oversampling techniques such as SMOTE and Autoencoders as well as with state of the art GAN-based models.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1901.02514 [cs, stat]},
	author = {Ger, Stephanie and Klabjan, Diego},
	month = aug,
	year = {2020},
	note = {arXiv: 1901.02514},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ger and Klabjan - 2020 - Autoencoders and Generative Adversarial Networks f.pdf:C\:\\Users\\kendr\\Zotero\\storage\\GNABUV3Y\\Ger and Klabjan - 2020 - Autoencoders and Generative Adversarial Networks f.pdf:application/pdf},
}

@article{zhao_monocular_2020,
	title = {Monocular {Depth} {Estimation} {Based} {On} {Deep} {Learning}: {An} {Overview}},
	volume = {63},
	issn = {1674-7321, 1869-1900},
	shorttitle = {Monocular {Depth} {Estimation} {Based} {On} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2003.06620},
	doi = {10.1007/s11431-020-1582-8},
	abstract = {Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semisupervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.},
	language = {en},
	number = {9},
	urldate = {2021-01-12},
	journal = {Science China Technological Sciences},
	author = {Zhao, Chaoqiang and Sun, Qiyu and Zhang, Chongzhen and Tang, Yang and Qian, Feng},
	month = sep,
	year = {2020},
	note = {arXiv: 2003.06620},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1612--1627},
	file = {Zhao et al. - 2020 - Monocular Depth Estimation Based On Deep Learning.pdf:C\:\\Users\\kendr\\Zotero\\storage\\JKX7FXEM\\Zhao et al. - 2020 - Monocular Depth Estimation Based On Deep Learning.pdf:application/pdf},
}

@article{dao_wireless_2015,
	title = {Wireless laptop-based phonocardiograph and diagnosis},
	volume = {3},
	issn = {2167-8359},
	url = {https://peerj.com/articles/1178},
	doi = {10.7717/peerj.1178},
	abstract = {Auscultation is used to evaluate heart health, and can indicate when it’s needed to refer a patient to a cardiologist. Advanced phonocardiograph (PCG) signal processing algorithms are developed to assist the physician in the initial diagnosis but they are primarily designed and demonstrated with research quality equipment. Therefore, there is a need to demonstrate the applicability of those techniques with consumer grade instrument. Furthermore, routine monitoring would benefit from a wireless PCG sensor that allows continuous monitoring of cardiac signals of patients in physical activity, e.g., treadmill or weight exercise. In this work, a low-cost portable and wireless healthcare monitoring system based on PCG signal is implemented to validate and evaluate the most advanced algorithms. Off-the-shelf electronics and a notebook PC are used with MATLAB codes to record and analyze PCG signals which are collected with a notebook computer in tethered and wireless mode. Physiological parameters based on the S1 and S2 signals and MATLAB codes are demonstrated.},
	language = {en},
	urldate = {2021-01-12},
	journal = {PeerJ},
	author = {Dao, Amy T.},
	month = aug,
	year = {2015},
	pages = {e1178},
	file = {Dao - 2015 - Wireless laptop-based phonocardiograph and diagnos.pdf:C\:\\Users\\kendr\\Zotero\\storage\\4Q5XM2FP\\Dao - 2015 - Wireless laptop-based phonocardiograph and diagnos.pdf:application/pdf},
}

@article{aziz_phonocardiogram_2020,
	title = {Phonocardiogram {Signal} {Processing} for {Automatic} {Diagnosis} of {Congenital} {Heart} {Disorders} through {Fusion} of {Temporal} and {Cepstral} {Features}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/13/3790},
	doi = {10.3390/s20133790},
	abstract = {Congenital heart disease (CHD) is a heart disorder associated with the devastating indications that result in increased mortality, increased morbidity, increased healthcare expenditure, and decreased quality of life. Ventricular Septal Defects (VSDs) and Arterial Septal Defects (ASDs) are the most common types of CHD. CHDs can be controlled before reaching a serious phase with an early diagnosis. The phonocardiogram (PCG) or heart sound auscultation is a simple and non-invasive technique that may reveal obvious variations of different CHDs. Diagnosis based on heart sounds is difﬁcult and requires a high level of medical training and skills due to human hearing limitations and the non-stationary nature of PCGs. An automated computer-aided system may boost the diagnostic objectivity and consistency of PCG signals in the detection of CHDs. The objective of this research was to assess the effects of various pattern recognition modalities for the design of an automated system that effectively differentiates normal, ASD, and VSD categories using short term PCG time series. The proposed model in this study adopts three-stage processing: pre-processing, feature extraction, and classiﬁcation. Empirical mode decomposition (EMD) was used to denoise the raw PCG signals acquired from subjects. One-dimensional local ternary patterns (1D-LTPs) and Mel-frequency cepstral coefﬁcients (MFCCs) were extracted from the denoised PCG signal for precise representation of data from different classes. In the ﬁnal stage, the fused feature vector of 1D-LTPs and MFCCs was fed to the support vector machine (SVM) classiﬁer using 10-fold cross-validation. The PCG signals were acquired from the subjects admitted to local hospitals and classiﬁed by applying various experiments. The proposed methodology achieves a mean accuracy of 95.24\% in classifying ASD, VSD, and normal subjects. The proposed model can be put into practice and serve as a second opinion for cardiologists by providing more objective and faster interpretations of PCG signals.},
	language = {en},
	number = {13},
	urldate = {2021-01-12},
	journal = {Sensors},
	author = {Aziz, Sumair and Khan, Muhammad Umar and Alhaisoni, Majed and Akram, Tallha and Altaf, Muhammad},
	month = jul,
	year = {2020},
	pages = {3790},
	file = {Aziz et al. - 2020 - Phonocardiogram Signal Processing for Automatic Di.pdf:C\:\\Users\\kendr\\Zotero\\storage\\7DPJ9HVT\\Aziz et al. - 2020 - Phonocardiogram Signal Processing for Automatic Di.pdf:application/pdf},
}

@inproceedings{bashar_heart_2018,
	address = {Sarawak, Malaysia},
	title = {Heart {Abnormality} {Classification} {Using} {Phonocardiogram} ({PCG}) {Signals}},
	isbn = {978-1-5386-2471-5},
	url = {https://ieeexplore.ieee.org/document/8626627/},
	doi = {10.1109/IECBES.2018.8626627},
	abstract = {Heart abnormality or disease is one of the leading causes of mortality worldwide. Sound signal produced by the mechanical activity of heart, known as phonocardiogram (PCG), provides useful information about the heart’s health. To increase discriminability among PCG signals of different normal and abnormal persons, an appropriate combination of signal features and classifiers is important. The segmentation of PCG signal, which requires corresponding ECG signal, is typically used for better prediction. But using ECG is generally expensive and time consuming. 781039In this paper, we therefore propose a segmentation free method to extract information from PCG signal. The signal is first preprocessed for DC removal and to limit the frequency to the required range. Four features (i.e. WPS, PS, FD, and SF) and four classifiers (i.e. LDA, ESVM, DT, and KNN) are then considered for the classification of heart murmur sound from PCG signals. A preliminary experiment with 56 signals showed the highest classification accuracy of 82.6\%, obtained by simple statistical feature (SF) with ESVM classifier. On average, the best performing classifier was ESVM (accuracy: 77.17\%), while the best feature was PS (accuracy: 75\%). In addition, the PS feature showed stable and consistent performance irrespective of the classifiers used. Results also indicate the importance of combining multiple features and classifiers for better accuracy and reliability.},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {2018 {IEEE}-{EMBS} {Conference} on {Biomedical} {Engineering} and {Sciences} ({IECBES})},
	publisher = {IEEE},
	author = {Bashar, Md. Khayrul and Dandapat, Samarendra and Kumazawa, Itsuo},
	month = dec,
	year = {2018},
	pages = {336--340},
	file = {Bashar et al. - 2018 - Heart Abnormality Classification Using Phonocardio.pdf:C\:\\Users\\kendr\\Zotero\\storage\\BKFV833W\\Bashar et al. - 2018 - Heart Abnormality Classification Using Phonocardio.pdf:application/pdf},
}

@inproceedings{potes_ensemble_2016,
	title = {Ensemble of {Feature}:based and {Deep} learning:based {Classifiers} for {Detection} of {Abnormal} {Heart} {Sounds}},
	shorttitle = {Ensemble of {Feature}},
	url = {http://www.cinc.org/archives/2016/pdf/182-399.pdf},
	doi = {10.22489/CinC.2016.182-399},
	abstract = {The goal of the 2016 PhysioNet/CinC Challenge is the development of an algorithm to classify normal/abnormal heart sounds. A total of 124 time-frequency features were extracted from the phonocardiogram (PCG) and input to a variant of the AdaBoost classiﬁer. A second classiﬁer using convolutional neural network (CNN) was trained using PCGs cardiac cycles decomposed into four frequency bands. The ﬁnal decision rule to classify normal/abnormal heart sounds was based on an ensemble of classiﬁers combining the outputs of AdaBoost and the CNN. The algorithm was trained on a training dataset (normal= 2575, abnormal= 665) and evaluated on a blind test dataset. Our classiﬁer ensemble approach obtained the highest score of the competition with a sensitivity, speciﬁcity, and overall score of 0.9424, 0.7781, and 0.8602, respectively.},
	language = {en},
	urldate = {2021-01-12},
	author = {Potes, Cristhian and Parvaneh, Saman and Rahman, Asif and Conroy, Bryan},
	month = sep,
	year = {2016},
	file = {Potes et al. - 2016 - Ensemble of Featurebased and Deep learningbased .pdf:C\:\\Users\\kendr\\Zotero\\storage\\PSVHYRJF\\Potes et al. - 2016 - Ensemble of Featurebased and Deep learningbased .pdf:application/pdf},
}

@article{noauthor_s1_2017,
	title = {S1 and {S2} {Heart} {Sound} {Recognition} {Using} {Deep} {Neural} {Networks}},
	volume = {64},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/7460939/},
	doi = {10.1109/TBME.2016.2559800},
	abstract = {Objective: This study focuses on the first (S1) and second (S2) heart sound recognition based only on acoustic characteristics; the assumptions of the individual durations of S1 and S2 and time intervals of S1–S2 and S2–S1 are not involved in the recognition process. The main objective is to investigate whether reliable S1 and S2 recognition performance can still be attained under situations where the duration and interval information might not be accessible. Methods: A deep neural network (DNN) method is proposed for recognizing S1 and S2 heart sounds. In the proposed method, heart sound signals are first converted into a sequence of Mel-frequency cepstral coefficients (MFCCs). The Kmeans algorithm is applied to cluster MFCC features into two groups to refine their representation and discriminative capability. The refined features are then fed to a DNN classifier to perform S1 and S2 recognition. We conducted experiments using actual heart sound signals recorded using an electronic stethoscope. Precision, recall, F-measure, and accuracy are used as the evaluation metrics. Results: The proposed DNN-based method can achieve high precision, recall, and F-measure scores with more than 91\% accuracy rate. Conclusion: The DNN classifier provides higher evaluation scores compared with other well-known pattern classification methods. Significance: The proposed DNN-based method can achieve reliable S1 and S2 recognition performance based on acoustic characteristics without using an ECG reference or incorporating the assumptions of the individual durations of S1 and S2 and time intervals of S1–S2 and S2–S1.},
	language = {en},
	number = {2},
	urldate = {2021-01-12},
	journal = {IEEE Transactions on Biomedical Engineering},
	month = feb,
	year = {2017},
	pages = {372--380},
	file = {2017 - S1 and S2 Heart Sound Recognition Using Deep Neura.pdf:C\:\\Users\\kendr\\Zotero\\storage\\NBYLILUG\\2017 - S1 and S2 Heart Sound Recognition Using Deep Neura.pdf:application/pdf},
}

@article{redlarski_system_2014,
	title = {A {System} for {Heart} {Sounds} {Classification}},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0112673},
	doi = {10.1371/journal.pone.0112673},
	abstract = {The future of quick and efficient disease diagnosis lays in the development of reliable non-invasive methods. As for the cardiac diseases – one of the major causes of death around the globe – a concept of an electronic stethoscope equipped with an automatic heart tone identification system appears to be the best solution. Thanks to the advancement in technology, the quality of phonocardiography signals is no longer an issue. However, appropriate algorithms for autodiagnosis systems of heart diseases that could be capable of distinguishing most of known pathological states have not been yet developed. The main issue is non-stationary character of phonocardiography signals as well as a wide range of distinguishable pathological heart sounds. In this paper a new heart sound classification technique, which might find use in medical diagnostic systems, is presented. It is shown that by combining Linear Predictive Coding coefficients, used for future extraction, with a classifier built upon combining Support Vector Machine and Modified Cuckoo Search algorithm, an improvement in performance of the diagnostic system, in terms of accuracy, complexity and range of distinguishable heart sounds, can be made. The developed system achieved accuracy above 93\% for all considered cases including simultaneous identification of twelve different heart sound classes. The respective system is compared with four different major classification methods, proving its reliability.},
	language = {en},
	number = {11},
	urldate = {2021-01-12},
	journal = {PLoS ONE},
	author = {Redlarski, Grzegorz and Gradolewski, Dawid and Palkowski, Aleksander},
	editor = {Talkachova, Alena},
	month = nov,
	year = {2014},
	pages = {e112673},
	file = {Redlarski et al. - 2014 - A System for Heart Sounds Classification.pdf:C\:\\Users\\kendr\\Zotero\\storage\\9A7G8LB7\\Redlarski et al. - 2014 - A System for Heart Sounds Classification.pdf:application/pdf},
}

@article{liu_open_2016,
	title = {An open access database for the evaluation of heart sound algorithms},
	volume = {37},
	issn = {0967-3334, 1361-6579},
	url = {https://iopscience.iop.org/article/10.1088/0967-3334/37/12/2181},
	doi = {10.1088/0967-3334/37/12/2181},
	language = {en},
	number = {12},
	urldate = {2021-01-12},
	journal = {Physiological Measurement},
	author = {Liu, Chengyu and Springer, David and Li, Qiao and Moody, Benjamin and Juan, Ricardo Abad and Chorro, Francisco J and Castells, Francisco and Roig, José Millet and Silva, Ikaro and Johnson, Alistair E W and Syed, Zeeshan and Schmidt, Samuel E and Papadaniil, Chrysa D and Hadjileontiadis, Leontios and Naseri, Hosein and Moukadem, Ali and Dieterlen, Alain and Brandt, Christian and Tang, Hong and Samieinasab, Maryam and Samieinasab, Mohammad Reza and Sameni, Reza and Mark, Roger G and Clifford, Gari D},
	month = dec,
	year = {2016},
	pages = {2181--2213},
	file = {Liu et al. - 2016 - An open access database for the evaluation of hear.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SKYBTCIW\\Liu et al. - 2016 - An open access database for the evaluation of hear.pdf:application/pdf},
}

@article{jia_direct_2019,
	title = {Direct speech-to-speech translation with a sequence-to-sequence model},
	url = {http://arxiv.org/abs/1904.06037},
	abstract = {We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and ﬁnd that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.},
	language = {en},
	urldate = {2021-01-13},
	journal = {arXiv:1904.06037 [cs, eess]},
	author = {Jia, Ye and Weiss, Ron J. and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.06037},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Jia et al. - 2019 - Direct speech-to-speech translation with a sequenc.pdf:C\:\\Users\\kendr\\Zotero\\storage\\GYUGQP8P\\Jia et al. - 2019 - Direct speech-to-speech translation with a sequenc.pdf:application/pdf},
}

@article{ali_denoising_2017,
	title = {Denoising of {Heart} {Sound} {Signals} {Using} {Discrete} {Wavelet} {Transform}},
	volume = {36},
	issn = {0278-081X, 1531-5878},
	url = {http://link.springer.com/10.1007/s00034-017-0524-7},
	doi = {10.1007/s00034-017-0524-7},
	abstract = {Signal denoising remains to be one of the main problems in the ﬁeld of signal processing. Various signal denoising algorithms using wavelet transforms have been introduced. Wavelets show superior signal denoising performance due to their properties such as multiresolution and windowing. This study focuses on denoising of phonocardiogram (PCG) signals using different families of discrete wavelet transforms, thresholding types and techniques, and signal decomposition levels. In particular, we discuss the effect of the chosen wavelet function and wavelet decomposition level on the efﬁciency of the denoising algorithm. Denoised signals are compared with the original PCG signal to determine the most suitable parameters (wavelet family, level of decomposition, and thresholding type) for the denoising process. The performance of our algorithm is evaluated using the signal-to-noise ratio, percentage root-mean-square difference, and root-mean-square error. The results show that the level of decomposition and thresholding type are the most important parameters affecting the efﬁciency of the denoising algorithm. Finally, we compare our results with those from other studies to test and optimize the performance of the proposed algorithm.},
	language = {en},
	number = {11},
	urldate = {2021-01-20},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Ali, Mohammed Nabih and El-Dahshan, EL-Sayed A. and Yahia, Ashraf H.},
	month = nov,
	year = {2017},
	pages = {4482--4497},
	file = {Ali et al. - 2017 - Denoising of Heart Sound Signals Using Discrete Wa.pdf:C\:\\Users\\kendr\\Zotero\\storage\\48WGA8RD\\Ali et al. - 2017 - Denoising of Heart Sound Signals Using Discrete Wa.pdf:application/pdf},
}

@inproceedings{babu_s1_2017,
	address = {Penang},
	title = {S1 and {S2} heart sound segmentation using variational mode decomposition},
	isbn = {978-1-5090-1134-6},
	url = {http://ieeexplore.ieee.org/document/8228119/},
	doi = {10.1109/TENCON.2017.8228119},
	abstract = {Heart sound segmentation is the process of identifying the various events like S1, S2, S3, S4, and murmurs present in the phonocardiogram (PCG) signal. In this paper, a heart sound segmentation algorithm for segmenting S1 and S2 heart sound which corresponds to the systolic and diastolic activity of the heart is presented. The proposed method is based on variational mode decomposition (VMD) algorithm. The recorded PCG is decomposed into various modes using VMD. A novel criterion is proposed to select an appropriate mode which is based on center frequency and relative energy. Then the Shannon entropy envelope of the selected mode is computed. S1 and S2 heart sound is segmented by applying dynamic threshold on the Shannon entropy envelope. The segmentation algorithm is tested on both available standard database (Physionet, Pascal, Michigan, and eGeneralMedical) and recorded signal. The simulation results demonstrate that the proposed method achieved the sensitivity, positive predictivity, and the accuracy of above 95\% with detection error rate of below 6\%.},
	language = {en},
	urldate = {2021-01-20},
	booktitle = {{TENCON} 2017 - 2017 {IEEE} {Region} 10 {Conference}},
	publisher = {IEEE},
	author = {Babu, K. Ajay and Ramkumar, Barathram and Manikandan, M. Sabarimalai},
	month = nov,
	year = {2017},
	pages = {1629--1634},
	file = {Babu et al. - 2017 - S1 and S2 heart sound segmentation using variation.pdf:C\:\\Users\\kendr\\Zotero\\storage\\TEVTGAA3\\Babu et al. - 2017 - S1 and S2 heart sound segmentation using variation.pdf:application/pdf},
}

@article{zhang_heart_2017,
	title = {Heart sound classification based on scaled spectrogram and partial least squares regression},
	volume = {32},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809416301616},
	doi = {10.1016/j.bspc.2016.10.004},
	language = {en},
	urldate = {2021-01-25},
	journal = {Biomedical Signal Processing and Control},
	author = {Zhang, Wenjie and Han, Jiqing and Deng, Shiwen},
	month = feb,
	year = {2017},
	pages = {20--28},
	file = {Zhang et al. - 2017 - Heart sound classification based on scaled spectro.pdf:C\:\\Users\\kendr\\Zotero\\storage\\JB9ZACBG\\Zhang et al. - 2017 - Heart sound classification based on scaled spectro.pdf:application/pdf},
}

@article{abo-zahhad_comparative_2016,
	title = {A comparative approach between cepstral features for human authentication using heart sounds},
	volume = {10},
	issn = {1863-1703, 1863-1711},
	url = {http://link.springer.com/10.1007/s11760-015-0826-9},
	doi = {10.1007/s11760-015-0826-9},
	abstract = {The main objective of this paper is to provide a comparative study between different cepstral features for the application of human recognition using heart sounds. In the past 10 years, heart sound, which is known as phonocardiogram, has been adopted for human biometric authentication tasks. Most of the previously proposed systems have adopted mel-frequency and linear frequency cepstral coefﬁcients as features for heart sounds. In this paper, two more cepstral features are proposed. The ﬁrst one is based on wavelet packet decomposition where a new ﬁlter bank structure is designed to select the appropriate bases for extracting discriminant features from heart sounds. The other is based on nonlinear modiﬁcation for mel-scaled cepstral features. The four cepstral features are tested and compared on two databases: One consists of 21 subjects, and the other consists of 206 subjects. Based on the achieved results over the two databases, the two proposed cepstral features achieved higher correct recognition rates and lower error rates in identiﬁcation and veriﬁcation modes, respectively.},
	language = {en},
	number = {5},
	urldate = {2021-01-25},
	journal = {Signal, Image and Video Processing},
	author = {Abo-Zahhad, M. and Farrag, Mohammed and Abbas, Sherif N. and Ahmed, Sabah M.},
	month = jul,
	year = {2016},
	pages = {843--851},
	file = {Abo-Zahhad et al. - 2016 - A comparative approach between cepstral features f.pdf:C\:\\Users\\kendr\\Zotero\\storage\\5CDFL755\\Abo-Zahhad et al. - 2016 - A comparative approach between cepstral features f.pdf:application/pdf},
}

@article{messner_heart_2018,
	title = {Heart {Sound} {Segmentation}—{An} {Event} {Detection} {Approach} {Using} {Deep} {Recurrent} {Neural} {Networks}},
	volume = {65},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/8370727/},
	doi = {10.1109/TBME.2018.2843258},
	abstract = {Objective: In this paper, we accurately detect the state-sequence ﬁrst heart sound (S1) - systole second heart sound (S2) - diastole, i.e. the positions of S1 and S2, in heart sound recordings. We propose an event detection approach, without explicitly incorporating a priori information of the state duration. This renders it also applicable to recordings with cardiac arrhythmia and extendable to the detection of extra heart sounds (third and fourth heart sound), heart murmurs, as well as other acoustic events. Methods: We use data from the 2016 PhysioNet/CinC Challenge, containing heart sound recordings and annotations of the heart sound states. From the recordings, we extract spectral and envelope features and investigate the performance of different deep recurrent neural network (DRNN) architectures to detect the state-sequence. We use virtual-adversarial training (VAT), dropout and data augmentation for regularization. Results: We compare our results with the state-of-the-art method and achieve an average score for the four events of the state-sequence of F1 ≈ 96\% on an independent test set. Conclusion: Our approach shows state-of-the-art performance carefully evaluated on the 2016 PhysioNet/CinC Challenge dataset. Signiﬁcance: In this work, we introduce a new methodology for the segmentation of heart sounds, suggesting an event detection approach with DRNNs using spectral or envelope features.},
	language = {en},
	number = {9},
	urldate = {2021-01-25},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Messner, Elmar and Zohrer, Matthias and Pernkopf, Franz},
	month = sep,
	year = {2018},
	pages = {1964--1974},
	file = {Messner et al. - 2018 - Heart Sound Segmentation—An Event Detection Approa.pdf:C\:\\Users\\kendr\\Zotero\\storage\\EY4IP9Q2\\Messner et al. - 2018 - Heart Sound Segmentation—An Event Detection Approa.pdf:application/pdf},
}

@article{esser_taming_2021,
	title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://compvis.github.io/taming-transformers/ .},
	urldate = {2021-02-18},
	journal = {arXiv:2012.09841 [cs]},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	month = feb,
	year = {2021},
	note = {arXiv: 2012.09841},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf:C\:\\Users\\kendr\\Zotero\\storage\\H3TMZS9J\\Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\XPFZ596E\\2012.html:text/html;Full Text:C\:\\Users\\kendr\\Zotero\\storage\\W7GKZ9NA\\Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf:application/pdf},
}

@misc{noauthor_photo_nodate,
	title = {Photo {Posters} - {Create} {Custom} {Photo} {Posters} {\textbar} {Walgreens} {Photo}},
	url = {https://photo.walgreens.com/store/poster-details?tab=photo_Posters-Posters#!/pdpview},
	urldate = {2021-02-18},
	file = {Photo Posters - Create Custom Photo Posters | Walgreens Photo:C\:\\Users\\kendr\\Zotero\\storage\\WEDTD3JV\\poster-details.html:text/html},
}

@article{jia_direct_2019-1,
	title = {Direct speech-to-speech translation with a sequence-to-sequence model},
	url = {http://arxiv.org/abs/1904.06037},
	abstract = {We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.},
	urldate = {2021-02-19},
	journal = {arXiv:1904.06037 [cs, eess]},
	author = {Jia, Ye and Weiss, Ron J. and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.06037},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Jia et al_2019_Direct speech-to-speech translation with a sequence-to-sequence model.pdf:C\:\\Users\\kendr\\Zotero\\storage\\Q9T7N7HG\\Jia et al_2019_Direct speech-to-speech translation with a sequence-to-sequence model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\WKQRHBVL\\1904.html:text/html},
}
