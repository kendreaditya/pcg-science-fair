
@article{bau_gan_2018,
	title = {{GAN} Dissection: Visualizing and Understanding Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1811.10597},
	shorttitle = {{GAN} Dissection},
	abstract = {Generative Adversarial Networks ({GANs}) have recently achieved impressive results for many real-world applications, and many {GAN} variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a {GAN} represent our visual world internally? What causes the artifacts in {GAN} results? How do architectural choices affect {GAN} learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand {GANs} at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving {GANs} by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their {GAN} models.},
	journaltitle = {{arXiv}:1811.10597 [cs]},
	author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B. and Freeman, William T. and Torralba, Antonio},
	urldate = {2021-01-10},
	date = {2018-12-08},
	eprinttype = {arxiv},
	eprint = {1811.10597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {Bau et al_2018_GAN Dissection.pdf:C\:\\Users\\kendr\\Zotero\\storage\\UFBZFCW7\\Bau et al_2018_GAN Dissection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\H9DTYGGY\\1811.html:text/html},
}

@book{d_detection_nodate,
	title = {Detection of Heart Diseases by Mathematical Artificial Intelligence Algorithm Using Phonocardiogram Signals},
	abstract = {Copyright © 2013 {ISSR} Journals. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. {ABSTRACT}: An artificial intelligence ({AI}) algorithm has been developed using Mathematical formula to diagnose heart disease from Phonocardiogram ({PCG}) signals. Auscultation, the technique of listening to heart sounds with a stethoscope can be used as a primary detection technique for detecting heart disorders for the past years. But now the Phonocardiogram, the digital recording of heart sounds is becoming very popular technique as it is relatively inexpensive. Four amplitude parameters of the {PCG} signal are extracted by using filter technique and are used as input. {PCG} signals for three types of heart diseases such as Tachycardia, Bradycardia and Atrial fibrillation were used in this paper to test the accuracy. These disease types that affect the electrical system of heart are known as arrhythmias, cause the heart to beat very fast (Tachycardia) or very slow (Bradycardia), or unexpectedly (Atrial fibrillation). After the signals are filtered and the parameters are extracted, the parameters are fed to the {AI} algorithm. Classifications of heart diseases are carried using the {AI} algorithm by comparing the extracted parameters. Here comparison is done using Min Max method. The developed mathematical artificial intelligence algorithm is implemented in {MATLab} using Simulink and the simulation results proved that the developed algorithm has been shown to be a powerful technique in detection of heart diseases using {PCG} signals.},
	author = {D, Prakash and T, Uma Mageshwari and K, Prabakaran and A, Suguna},
	file = {D et al_Detection of Heart Diseases by Mathematical Artificial Intelligence Algorithm.pdf:C\:\\Users\\kendr\\Zotero\\storage\\U5NRTT5V\\D et al_Detection of Heart Diseases by Mathematical Artificial Intelligence Algorithm.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\IAB52DHP\\download.html:text/html},
}

@online{noauthor_physionetcinc_nodate,
	title = {{PhysioNet}/{CinC} Challenge 2016: Training Sets},
	url = {https://archive.physionet.org/pn3/challenge/2016/},
	urldate = {2021-01-11},
	file = {PhysioNet/CinC Challenge 2016\: Training Sets:C\:\\Users\\kendr\\Zotero\\storage\\KSU4Z6PL\\2016.html:text/html},
}

@article{rawther_detection_2015,
	title = {Detection and Classification of Cardiac Arrhythmias based on {ECG} and {PCG} using Temporal and Wavelet features},
	volume = {4},
	abstract = {Arrhythmias are abnormal rhythms of heart. Sudden Cardiac Arrest is most often caused by life threatening arrhythmias such as Ventricular Tachycardia ({VT}) and Ventricular Fibrillation ({VF}). Early detection of life threatening arrhythmias is crucial for successive defibrillation therapy. In general heart diseases have been investigated by various methods. Among these Electrocardiography ({ECG}) test is considered as the best noninvasive method of investigation. {ECG} test is varied if the heart sound from the Phonocardiogram shows any abnormalities. So Phonocardiography ({PCG}) is also considered for more efficiency. Commonly used arrhythmia detection and classification algorithms are only based on surface Electrocardiogram analysis. So an algorithm corresponds to multiresolution wavelet analysis using temporal and wavelet features of Electrocardiogram and Phonocardiogram along with Electrocardiogram-Phonocardiogram relationships is designed so as to increase the efficiency of the heart diagnostics. Temporal and wavelet features of {ECG} and {PCG} along with the linear prediction coefficients representing {ECG} and {PCG} are fed to the classifier for classification. The goal of this work is to achieve an efficient arrhythmia detection system that can lead to high performance heart diagnostics.},
	pages = {6},
	number = {4},
	author = {Rawther, Nabina N and Cheriyan, Jini},
	date = {2015},
	langid = {english},
	file = {Rawther and Cheriyan - 2015 - Detection and Classification of Cardiac Arrhythmia.pdf:C\:\\Users\\kendr\\Zotero\\storage\\Q2B7GTV6\\Rawther and Cheriyan - 2015 - Detection and Classification of Cardiac Arrhythmia.pdf:application/pdf},
}

@article{ismail_localization_2018,
	title = {Localization and classification of heart beats in phonocardiography signals —a comprehensive review},
	volume = {2018},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-018-0545-9},
	doi = {10.1186/s13634-018-0545-9},
	abstract = {Phonocardiogram ({PCG}) signal represents recording of sounds and murmurs resulting from heart auscultation. Analysis of these {PCG} signals is critical in diagnosis of different heart diseases. Over the years, a variety of methods have been proposed for automatic analysis of {PCG} signals in time, frequency, and time-frequency domains. This paper presents a comprehensive survey of different methods proposed for automatic analysis of {PCG} signals with the objective to evaluate the current state-of-the-art and to determine the potential domains of effective analysis. An important aspect of our contribution is that the review is carried out as a function of domains of analysis rather than simply discussing different methods. Our method further splits analysis into pre-processing, localization, and classification, and details are presented in terms of techniques and classifiers used during these phases. Finally, results are summarized for normal heart beat, noisy heart beat, and different pathologies using metrices like accuracy and detection rate. In addition to time and frequency domain, time-frequency based methods including wavelet, empirical mode decomposition ({EMD}) and time-frequency representation ({TFR}) are selected for detailed analysis. The review concludes that the time-frequency representations like {EMD} and wavelets represent areas of exploration in future along with perspective of using different time-frequency techniques together.},
	pages = {26},
	number = {1},
	journaltitle = {{EURASIP} J. Adv. Signal Process.},
	author = {Ismail, Shahid and Siddiqi, Imran and Akram, Usman},
	urldate = {2021-01-12},
	date = {2018-12},
	langid = {english},
	file = {Ismail et al. - 2018 - Localization and classification of heart beats in .pdf:C\:\\Users\\kendr\\Zotero\\storage\\RSE7C8IF\\Ismail et al. - 2018 - Localization and classification of heart beats in .pdf:application/pdf},
}

@article{deperlioglu_diagnosis_2020,
	title = {Diagnosis of heart diseases by a secure Internet of Health Things system based on Autoencoder Deep Neural Network},
	volume = {162},
	issn = {01403664},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140366420318909},
	doi = {10.1016/j.comcom.2020.08.011},
	abstract = {Objective of this study is to introduce a secure {IoHT} system, which acts as a clinical decision support system with the diagnosis of cardiovascular diseases. In this sense, it was emphasized that the accuracy rate of diagnosis (classification) can be improved via deep learning algorithms, by needing no hybrid-complex models, and a secure data processing can be achieved with a multi-authentication and Tangle based approach. In detail, heart sounds were classified with Autoencoder Neural Networks ({AEN}) and the {IoHT} system was built for supporting doctors in real-time. For developing the diagnosis infrastructure by the {AEN}, {PASCAL} B-Training and Physiobank-{PhysioNet} A-Training heart sound datasets were used accordingly. For the {PASCAL} dataset, the {AEN} provided a diagnosis-classification performance with the accuracy of 100\%, sensitivity of 100\%, and the specificity of 100\% whereas the rates were respectively 99.8\%, 99.65\%, and 99.13\% for the {PhysioNet} dataset. It was seen that the findings by the developed {AEN} based solution were better than the alternative solutions from the literature. Additionally, usability of the whole {IoHT} system was found positive by the doctors, and according to the 479 real-case applications, the system was able to achieve accuracy rates of 96.03\% for normal heart sounds, 91.91\% for extrasystole, and 90.11\% for murmur. In terms of security approach, the system was also robust against several attacking methods including synthetic data impute as well as trying to penetrating to the system via central system or mobile devices.},
	pages = {31--50},
	journaltitle = {Computer Communications},
	author = {Deperlioglu, Omer and Kose, Utku and Gupta, Deepak and Khanna, Ashish and Sangaiah, Arun Kumar},
	urldate = {2021-01-12},
	date = {2020-10},
	langid = {english},
	file = {Deperlioglu et al. - 2020 - Diagnosis of heart diseases by a secure Internet o.pdf:C\:\\Users\\kendr\\Zotero\\storage\\7GJS6QXK\\Deperlioglu et al. - 2020 - Diagnosis of heart diseases by a secure Internet o.pdf:application/pdf},
}

@article{surukutla_cardiac_2020,
	title = {Cardiac Arrhythmia Detection Using {CNN}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3572237},
	doi = {10.2139/ssrn.3572237},
	abstract = {The diagnosis of cardiac arrhythmias can be a tedious process when done by hand and could benefit greatly from computer automation. To this end, an algorithm was developed to distinguish between normal heart beats and abnormal arrhythmic beats in an {PCG} Recording. First an algorithm was developed to find the location of {QRS} complexes in the {PCG} Recording. Principal component analysis was performed using the area around the {QRS} complex. 20 of the resulting principal components were used to train a simple linear classifier to distinguish between normal and abnormal beats. The classification performed reasonably well with a sensitivity of 85.4\% and specificity of 91.7\%. Moresophisticated signal processing and classification techniques could be applied to improve these numbers, but the algorithm is a good starting point.},
	journaltitle = {{SSRN} Journal},
	author = {Surukutla, Dinesh and Bhanushali, Karan and Patil, Trupti},
	urldate = {2021-01-12},
	date = {2020},
	langid = {english},
	file = {Surukutla et al. - 2020 - Cardiac Arrhythmia Detection Using CNN.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SSWCKM2S\\Surukutla et al. - 2020 - Cardiac Arrhythmia Detection Using CNN.pdf:application/pdf},
}

@incollection{subasi_biomedical_2019,
	title = {Biomedical Signals},
	isbn = {978-0-12-817444-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128174449000027},
	pages = {27--87},
	booktitle = {Practical Guide for Biomedical Signals Analysis Using Machine Learning Techniques},
	publisher = {Elsevier},
	author = {Subasi, Abdulhamit},
	urldate = {2021-01-12},
	date = {2019},
	langid = {english},
	doi = {10.1016/B978-0-12-817444-9.00002-7},
	file = {Subasi - 2019 - Biomedical Signals.pdf:C\:\\Users\\kendr\\Zotero\\storage\\ACZ2RXSR\\Subasi - 2019 - Biomedical Signals.pdf:application/pdf},
}

@inproceedings{zhou_learning_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Learning Deep Features for Discriminative Localization},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780688/},
	doi = {10.1109/CVPR.2016.319},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network ({CNN}) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we ﬁnd that it actually builds a generic localizable deep representation that exposes the implicit attention of {CNNs} on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on {ILSVRC} 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classiﬁcation task1.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2921--2929},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	urldate = {2021-01-12},
	date = {2016-06},
	langid = {english},
	file = {Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf:C\:\\Users\\kendr\\Zotero\\storage\\X6A45GK6\\Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf:application/pdf},
}

@article{krishnan_automated_2020,
	title = {Automated heart sound classification system from unsegmented phonocardiogram ({PCG}) using deep neural network},
	volume = {43},
	issn = {2662-4729, 2662-4737},
	url = {http://link.springer.com/10.1007/s13246-020-00851-w},
	doi = {10.1007/s13246-020-00851-w},
	abstract = {Given the patient to doctor ratio of 50,000:1 in low income and middle-income countries, there is a need for automated heart sound classification system that can screen the Phonocardiogram ({PCG}) records in real-time. This paper proposes deep neural network architectures such as a one-dimensional convolutional neural network (1D-{CNN}) and Feed-forward Neural Network (F-{NN}) for the classification of unsegmented phonocardiogram ({PCG}) signal. The research paper aims to automate the feature engineering and feature selection process used in the analysis of the {PCG} signal. The original {PCG} signal is down-sampled at 500 Hz. Then they are divided into smaller time segments of 6 s epochs. Savitzky–Golay filter is used to suppress the high-frequency noises in the signal by data point smoothening. The processed data was then provided as an input to the proposed deep neural network ({DNN}) architectures. 1081 {PCG} records were used for training and validating the proposed {DNN} models. The Feed-forward Neural Network model with five hidden layers provided a better overall accuracy of 0.8565 with a sensitivity of 0.8673, and specificity of 0.8475. The balanced accuracy of the model was found to be 0.8574. The performance of the model was also studied using the Receiver Operating Characteristic ({ROC}) plot, which produced an Area Under the Curve ({AUC}) value of 0.857. The classification accuracy of the proposed models was compared to the related works on {PCG} signal analysis for cardiovascular disease detection. The {DNN} models studied in this study provided comparable performance in heart sound classification without the requirement of feature engineering and segmentation of heart sound signals.},
	pages = {505--515},
	number = {2},
	journaltitle = {Phys Eng Sci Med},
	author = {Krishnan, Palani Thanaraj and Balasubramanian, Parvathavarthini and Umapathy, Snekhalatha},
	urldate = {2021-01-12},
	date = {2020-06},
	langid = {english},
	file = {Krishnan et al. - 2020 - Automated heart sound classification system from u.pdf:C\:\\Users\\kendr\\Zotero\\storage\\UJ2U4CM9\\Krishnan et al. - 2020 - Automated heart sound classification system from u.pdf:application/pdf},
}

@article{das_human_2016,
	title = {Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?},
	url = {http://arxiv.org/abs/1606.03556},
	shorttitle = {Human Attention in Visual Question Answering},
	abstract = {We conduct large-scale studies on ‘human attention’ in Visual Question Answering ({VQA}) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the {VQA}-{HAT} (Human {ATtention}) dataset. We evaluate attention maps generated by state-of-the-art {VQA} models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in {VQA} do not seem to be looking at the same regions as humans.},
	journaltitle = {{arXiv}:1606.03556 [cs]},
	author = {Das, Abhishek and Agrawal, Harsh and Zitnick, C. Lawrence and Parikh, Devi and Batra, Dhruv},
	urldate = {2021-01-12},
	date = {2016-06-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.03556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Das et al. - 2016 - Human Attention in Visual Question Answering Do H.pdf:C\:\\Users\\kendr\\Zotero\\storage\\DD59S5CC\\Das et al. - 2016 - Human Attention in Visual Question Answering Do H.pdf:application/pdf},
}

@article{rajpurkar_cardiologist-level_2017,
	title = {Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1707.01836},
	abstract = {We develop an algorithm which exceeds the performance of board certiﬁed cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of {ECG} samples to a sequence of rhythm classes. Committees of boardcertiﬁed cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value).},
	journaltitle = {{arXiv}:1707.01836 [cs]},
	author = {Rajpurkar, Pranav and Hannun, Awni Y. and Haghpanahi, Masoumeh and Bourn, Codie and Ng, Andrew Y.},
	urldate = {2021-01-12},
	date = {2017-07-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1707.01836},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Rajpurkar et al. - 2017 - Cardiologist-Level Arrhythmia Detection with Convo.pdf:C\:\\Users\\kendr\\Zotero\\storage\\YHQTNL4C\\Rajpurkar et al. - 2017 - Cardiologist-Level Arrhythmia Detection with Convo.pdf:application/pdf},
}

@article{fernando_heart_2020,
	title = {Heart Sound Segmentation using Bidirectional {LSTMs} with Attention},
	volume = {24},
	issn = {2168-2194, 2168-2208},
	url = {http://arxiv.org/abs/2004.03712},
	doi = {10.1109/JBHI.2019.2949516},
	abstract = {Objective: This paper proposes a novel framework for the segmentation of phonocardiogram ({PCG}) signals into heart states, exploiting the temporal evolution of the {PCG} as well as considering the salient information that it provides for the detection of the heart state. Methods: We propose the use of recurrent neural networks and exploit recent advancements in attention based learning to segment the {PCG} signal. This allows the network to identify the most salient aspects of the signal and disregard uninformative information. Results: The proposed method attains state-of-the-art performance on multiple benchmarks including both human and animal heart recordings. Furthermore, we empirically analyse different feature combinations including envelop features, wavelet and Mel Frequency Cepstral Coefﬁcients ({MFCC}), and provide quantitative measurements that explore the importance of different features in the proposed approach. Conclusion: We demonstrate that a recurrent neural network coupled with attention mechanisms can effectively learn from irregular and noisy {PCG} recordings. Our analysis of different feature combinations shows that {MFCC} features and their derivatives offer the best performance compared to classical wavelet and envelop features. Signiﬁcance: Heart sound segmentation is a crucial pre-processing step for many diagnostic applications. The proposed method provides a cost effective alternative to labour extensive manual segmentation, and provides a more accurate segmentation than existing methods. As such, it can improve the performance of further analysis including the detection of murmurs and ejection clicks. The proposed method is also applicable for detection and segmentation of other one dimensional biomedical signals.},
	pages = {1601--1609},
	number = {6},
	journaltitle = {{IEEE} J. Biomed. Health Inform.},
	author = {Fernando, Tharindu and Ghaemmaghami, Houman and Denman, Simon and Sridharan, Sridha and Hussain, Nayyar and Fookes, Clinton},
	urldate = {2021-01-12},
	date = {2020-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.03712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, Quantitative Biology - Quantitative Methods},
	file = {Fernando et al. - 2020 - Heart Sound Segmentation using Bidirectional LSTMs.pdf:C\:\\Users\\kendr\\Zotero\\storage\\QNTW2APB\\Fernando et al. - 2020 - Heart Sound Segmentation using Bidirectional LSTMs.pdf:application/pdf},
}

@inproceedings{almasi_dynamical_2011,
	location = {Boston, {MA}},
	title = {A dynamical model for generating synthetic Phonocardiogram signals},
	isbn = {978-1-4577-1589-1 978-1-4244-4121-1 978-1-4244-4122-8},
	url = {http://ieeexplore.ieee.org/document/6091376/},
	doi = {10.1109/IEMBS.2011.6091376},
	abstract = {In this paper we introduce a dynamical model for Phonocardiogram ({PCG}) signal which is capable of generating realistic synthetic {PCG} signals. This model is based on {PCG} morphology and consists of three ordinary differential equations and can represent various morphologies of normal {PCG} signals. Beat-to-beat variation in {PCG} morphology is significant so model parameters vary from beat to beat. This model is inspired of Electrocardiogram ({ECG}) dynamical model proposed by {McSharry} et al. and can be employed to assess biomedical signal processing techniques.},
	eventtitle = {2011 33rd Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society},
	pages = {5686--5689},
	booktitle = {2011 Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society},
	publisher = {{IEEE}},
	author = {Almasi, A. and Shamsollahi, M. B. and Senhadji, L.},
	urldate = {2021-01-12},
	date = {2011-08},
	langid = {english},
	file = {Almasi et al. - 2011 - A dynamical model for generating synthetic Phonoca.pdf:C\:\\Users\\kendr\\Zotero\\storage\\RYXT4H8E\\Almasi et al. - 2011 - A dynamical model for generating synthetic Phonoca.pdf:application/pdf},
}

@article{kalaivani_diagnosis_2014,
	title = {{DIAGNOSIS} {OF} {ARRHYTHMIA} {DISEASES} {USING} {HEART} {SOUNDS} {AND} {ECG} {SIGNALS}},
	issn = {1560-4071},
	url = {http://russjcardiol.elpub.ru/jour/article/view/590},
	doi = {10.15829/1560-4071-2014-1-ENG-35-41},
	pages = {35--41},
	number = {1},
	journaltitle = {Russ J Cardiol},
	author = {Kalaivani, V.},
	urldate = {2021-01-12},
	date = {2014-01-01},
	langid = {english},
	file = {Kalaivani - 2014 - DIAGNOSIS OF ARRHYTHMIA DISEASES USING HEART SOUND.pdf:C\:\\Users\\kendr\\Zotero\\storage\\IY5WHD2L\\Kalaivani - 2014 - DIAGNOSIS OF ARRHYTHMIA DISEASES USING HEART SOUND.pdf:application/pdf},
}

@article{olah_feature_2017,
	title = {Feature Visualization},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	pages = {10.23915/distill.00007},
	number = {11},
	journaltitle = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	urldate = {2021-01-12},
	date = {2017-11-07},
	langid = {english},
	file = {Olah et al. - 2017 - Feature Visualization.pdf:C\:\\Users\\kendr\\Zotero\\storage\\M8BE9DVH\\Olah et al. - 2017 - Feature Visualization.pdf:application/pdf},
}

@article{finlay_mobile-phonocardiogram_2006,
	title = {The "mobile-phonocardiogram", a new tool in the arrhythmia clinic},
	volume = {92},
	issn = {1355-6037},
	url = {https://heart.bmj.com/lookup/doi/10.1136/hrt.2005.076315},
	doi = {10.1136/hrt.2005.076315},
	pages = {898--898},
	number = {7},
	journaltitle = {Heart},
	author = {Finlay, M},
	urldate = {2021-01-12},
	date = {2006-05-02},
	langid = {english},
	file = {Finlay - 2006 - The mobile-phonocardiogram, a new tool in the ar.pdf:C\:\\Users\\kendr\\Zotero\\storage\\EBZ3J29W\\Finlay - 2006 - The mobile-phonocardiogram, a new tool in the ar.pdf:application/pdf},
}

@inproceedings{garg_heart_2019,
	location = {Noida, India},
	title = {Heart Rhythm Abnormality Detection from {PCG} Signal},
	isbn = {978-1-72813-591-5},
	url = {https://ieeexplore.ieee.org/document/8844950/},
	doi = {10.1109/IC3.2019.8844950},
	abstract = {Cardiovascular diseases are the major cause of mortality and {PCG} provides a non-invasive way to monitor the heart. In this paper, we give a unique approach to classify Normal and Abnormal heart rhythms using Machine Learning. The heart sounds are digital signals recorded from electronic stethoscope. In the initial phase Signal Quality assessment and feature extraction is done after which we explore different data models to find the relation between the features and the results, achieving poor results. In the second phase, audio files are segmented into Systolic and Diastolic phases using a Logistic Regression-{HSMM} These segments of systoles and diastoles are then individually analyzed and individual feature extraction is done. In the segmentation process a lot of de-noising is also done removing the background noises. This approach yields an accuracy of 79\% which concludes that analyzing the heart signal at Systolic and Diastolic phase is a very essential step to solve this problem.},
	eventtitle = {2019 Twelfth International Conference on Contemporary Computing ({IC}3)},
	pages = {1--5},
	booktitle = {2019 Twelfth International Conference on Contemporary Computing ({IC}3)},
	publisher = {{IEEE}},
	author = {Garg, Varsha and Mathur, Arpit and Mangla, Nishant and Rawat, Aman Singh},
	urldate = {2021-01-12},
	date = {2019-08},
	langid = {english},
	file = {Garg et al. - 2019 - Heart Rhythm Abnormality Detection from PCG Signal.pdf:C\:\\Users\\kendr\\Zotero\\storage\\MLDBMTQR\\Garg et al. - 2019 - Heart Rhythm Abnormality Detection from PCG Signal.pdf:application/pdf},
}

@article{jordaens_clinical_2018,
	title = {A clinical approach to arrhythmias revisited in 2018: From {ECG} over noninvasive and invasive electrophysiology to advanced imaging},
	volume = {26},
	issn = {1568-5888, 1876-6250},
	url = {http://link.springer.com/10.1007/s12471-018-1089-1},
	doi = {10.1007/s12471-018-1089-1},
	shorttitle = {A clinical approach to arrhythmias revisited in 2018},
	abstract = {Understanding arrhythmias and their treatment is not always easy. The current straightforward approach with catheter ablation and device therapy is an amazing achievement, but does not make management of underlying or other cardiac disease and pharmacological therapy unnecessary. The goal of this paper is to describe how much of the knowledge of the 1980s and early 1990s can and should still be applied in the modern treatment of patients with arrhythmias. After an introduction, this review will focus on paroxysmal atrial ﬁbrillation and a prototype of ‘idiopathic’ ventricular arrhythmias, two diseases with a striking similarity, and will discuss the arrhythmogenesis. The {ECG} continues to play an important role in diagnostics. Both diseases are associated with a structurally normal heart; the autonomic nervous system plays an important role in triggering arrhythmias at both the atrial and ventricular level.},
	pages = {182--189},
	number = {4},
	journaltitle = {Neth Heart J},
	author = {Jordaens, L.},
	urldate = {2021-01-12},
	date = {2018-04},
	langid = {english},
	file = {Jordaens - 2018 - A clinical approach to arrhythmias revisited in 20.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SVC7LVTY\\Jordaens - 2018 - A clinical approach to arrhythmias revisited in 20.pdf:application/pdf},
}

@article{kiranyaz_real-time_2020,
	title = {Real-time phonocardiogram anomaly detection by adaptive 1D Convolutional Neural Networks},
	volume = {411},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220309085},
	doi = {10.1016/j.neucom.2020.05.063},
	abstract = {The heart sound signals (Phonocardiogram – {PCG}) enable the earliest monitoring to detect a potential cardiovascular pathology and have recently become a crucial tool as a diagnostic test in outpatient monitoring to assess heart hemodynamic status. The need for an automated and accurate anomaly detection method for {PCG} has thus become imminent. To determine the state-of-the-art {PCG} classiﬁcation algorithm, 48 international teams competed in the {PhysioNet} ({CinC}) Challenge in 2016 over the largest benchmark dataset with 3126 records with the classiﬁcation outputs, normal (N), abnormal (A) and unsure – too noisy (U). In this study, our aim is to push this frontier further; however, we focus deliberately on the anomaly detection problem while assuming a reasonably high Signal-to-Noise Ratio ({SNR}) on the records. By using 1D Convolutional Neural Networks trained with a novel data puriﬁcation approach, we aim to achieve the highest detection performance and real-time processing ability with signiﬁcantly lower delay and computational complexity. The experimental results over the high-quality subset of the same benchmark dataset show that the proposed approach achieves both objectives. Furthermore, our ﬁndings reveal the fact that further improvements indeed require a personalized (patient-speciﬁc) approach to avoid major drawbacks of a global {PCG} classiﬁcation approach.},
	pages = {291--301},
	journaltitle = {Neurocomputing},
	author = {Kiranyaz, Serkan and Zabihi, Morteza and Rad, Ali Bahrami and Ince, Turker and Hamila, Ridha and Gabbouj, Moncef},
	urldate = {2021-01-12},
	date = {2020-10},
	langid = {english},
	file = {Kiranyaz et al. - 2020 - Real-time phonocardiogram anomaly detection by ada.pdf:C\:\\Users\\kendr\\Zotero\\storage\\TTYMRV6T\\Kiranyaz et al. - 2020 - Real-time phonocardiogram anomaly detection by ada.pdf:application/pdf},
}

@article{lubaib_heart_2016,
	title = {The Heart Defect Analysis Based on {PCG} Signals Using Pattern Recognition Techniques},
	volume = {24},
	issn = {22120173},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212017316303164},
	doi = {10.1016/j.protcy.2016.05.225},
	abstract = {The graphical recording of the heart sounds and murmurs is called Phonocardiogram or {PCG} and the machine is so called phonocardiograph. It has an important role in the proper and accurate diagnosis of the heart defects. It requires highly and experienced professionals to read the phonocardiogram, as usually with the stethoscope. The paper is about the implementation of a diagnostic system as a detector and classifier; for heart diseases. Various heart sound samples are classified using Support Vector Machine ({SVM}), K Nearest Neighbour ({KNN}), Bayesian and Gaussian Mixture Model ({KNN}) Classifiers. The output of the system is the classification of the sound as either normal or abnormal and if it is abnormal, what type of abnormality is present. In the proposed method, time domain and frequency domain features are extracted. Various frequency domain features such as energy, mean, variance and Mel Frequency Cepstral Coefficients ({MFCC}) are analysed.},
	pages = {1024--1031},
	journaltitle = {Procedia Technology},
	author = {Lubaib, P. and Muneer, K.V. Ahammed},
	urldate = {2021-01-12},
	date = {2016},
	langid = {english},
	file = {Lubaib and Muneer - 2016 - The Heart Defect Analysis Based on PCG Signals Usi.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SSC4JC38\\Lubaib and Muneer - 2016 - The Heart Defect Analysis Based on PCG Signals Usi.pdf:application/pdf},
}

@article{thoms_real-life_2019,
	title = {Real-life physics: phonocardiography, electrocardiography, and audiometry with a smartphone},
	volume = {1223},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1223/1/012007},
	doi = {10.1088/1742-6596/1223/1/012007},
	shorttitle = {Real-life physics},
	abstract = {To foster student motivation and engagement, we combined authentic contexts, procedures, and materials by assessing biomedical physics topics with a smartphone. Selected experiments with simple aids allow for the examination of a student’s heartbeat in various ways: e.g., phonocardiography and electrocardiography. In addition, students can test their frequency-dependent hearing threshold. These contexts lead to an understanding of various physics concepts in a meaningful way.},
	pages = {012007},
	journaltitle = {J. Phys.: Conf. Ser.},
	author = {Thoms, Lars-Jochen and Collichia, Giuseppe and Girwidz, Raimund},
	urldate = {2021-01-12},
	date = {2019-05},
	langid = {english},
	file = {Thoms et al. - 2019 - Real-life physics phonocardiography, electrocardi.pdf:C\:\\Users\\kendr\\Zotero\\storage\\IMBDIDLD\\Thoms et al. - 2019 - Real-life physics phonocardiography, electrocardi.pdf:application/pdf},
}

@article{yupapin_heart_2011,
	title = {Heart detection and diagnosis based on {ECG} and {EPCG} relationships},
	issn = {1179-1470},
	url = {http://www.dovepress.com/heart-detection-and-diagnosis-based-on-ecg-and-epcg-relationships-peer-reviewed-article-MDER},
	doi = {10.2147/MDER.S23324},
	abstract = {A new design of a system for preliminary detection of defective hearts is proposed which is composed of two subsystems, in which one is based on the relationship between the electrocardiogram ({ECG}) and phonocardiogram ({PCG}) signals. The relationship between both signals is determined as an impulse response (h(n)) of a system, where the decision is made based on the linear predictive coding coefficients of a heart’s impulse response. The other subsystem uses a phase space approach, in which the mean squared error between the distance vectors of the phase space of the normal heart and abnormal heart is judged by the likelihood ratio test (Λ) value, on which the decision is made. The advantage of the proposed system is that a heart’s diagnosis system based on the {ECG} and {EPCG} signals can lead to high performance heart diagnostics.},
	pages = {133},
	journaltitle = {{MDER}},
	author = {Yupapin, Preecha and {Wardkein} and Yupapin, Preecha and {Phanphaisarn} and {Koseeyaporn} and {Roeksabutr} and {Roeksabutr} and {Wardkein} and {Koseeyapon}},
	urldate = {2021-01-12},
	date = {2011-08},
	langid = {english},
	file = {Yupapin et al. - 2011 - Heart detection and diagnosis based on ECG and EPC.pdf:C\:\\Users\\kendr\\Zotero\\storage\\FMP7KD4P\\Yupapin et al. - 2011 - Heart detection and diagnosis based on ECG and EPC.pdf:application/pdf},
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and Understanding Convolutional Networks},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the {ImageNet} benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the {ImageNet} classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our {ImageNet} model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journaltitle = {{arXiv}:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	urldate = {2021-01-12},
	date = {2013-11-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:C\:\\Users\\kendr\\Zotero\\storage\\XT38YXCF\\Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	journaltitle = {{arXiv}:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2021-01-12},
	date = {2016-09-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:C\:\\Users\\kendr\\Zotero\\storage\\JRDWYLE5\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf},
}

@article{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	url = {http://arxiv.org/abs/1706.03825},
	shorttitle = {{SmoothGrad}},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classiﬁer, one type of explanation is to identify pixels that strongly inﬂuence the ﬁnal decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces {SMOOTHGRAD}, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	journaltitle = {{arXiv}:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	urldate = {2021-01-12},
	date = {2017-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:C\:\\Users\\kendr\\Zotero\\storage\\8S3LTZYZ\\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:application/pdf},
}

@article{latif_phonocardiographic_2020,
	title = {Phonocardiographic Sensing using Deep Learning for Abnormal Heartbeat Detection},
	url = {http://arxiv.org/abs/1801.08322},
	abstract = {Cardiac auscultation involves expert interpretation of abnormalities in heart sounds using stethoscope. Deep learning based cardiac auscultation is of signiﬁcant interest to the healthcare community as it can help reducing the burden of manual auscultation with automated detection of abnormal heartbeats. However, the problem of automatic cardiac auscultation is complicated due to the requirement of reliability and high accuracy, and due to the presence of background noise in the heartbeat sound. In this work, we propose a Recurrent Neural Networks ({RNNs}) based automated cardiac auscultation solution. Our choice of {RNNs} is motivated by the great success of deep learning in medical applications and by the observation that {RNNs} represent the deep learning conﬁguration most suitable for dealing with sequential or temporal data even in the presence of noise. We explore the use of various {RNN} models, and demonstrate that these models deliver the abnormal heartbeat classiﬁcation score with signiﬁcant improvement. Our proposed approach using {RNNs} can be potentially be used for real-time abnormal heartbeat detection in the Internet of Medical Things for remote monitoring applications.},
	journaltitle = {{arXiv}:1801.08322 [cs]},
	author = {Latif, Siddique and Usman, Muhammad and Rana, Rajib and Qadir, Junaid},
	urldate = {2021-01-12},
	date = {2020-07-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.08322},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Latif et al. - 2020 - Phonocardiographic Sensing using Deep Learning for.pdf:C\:\\Users\\kendr\\Zotero\\storage\\LDJZQYZ5\\Latif et al. - 2020 - Phonocardiographic Sensing using Deep Learning for.pdf:application/pdf},
}

@article{ger_autoencoders_2020,
	title = {Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification},
	url = {http://arxiv.org/abs/1901.02514},
	abstract = {Generative Adversarial Networks ({GANs}) have been used in many different applications to generate realistic synthetic data. We introduce a novel {GAN} with Autoencoder ({GAN}-{AE}) architecture to generate synthetic samples for variable length, multi-feature sequence datasets. In this model, we develop a {GAN} architecture with an additional autoencoder component, where recurrent neural networks ({RNNs}) are used for each component of the model in order to generate synthetic data to improve classiﬁcation accuracy for a highly imbalanced medical device dataset. In addition to the medical device dataset, we also evaluate the {GAN}-{AE} performance on two additional datasets and demonstrate the application of {GAN}-{AE} to a sequence-to-sequence task where both synthetic sequence inputs and sequence outputs must be generated. To evaluate the quality of the synthetic data, we train encoder-decoder models both with and without the synthetic data and compare the classiﬁcation model performance. We show that a model trained with {GAN}-{AE} generated synthetic data outperforms models trained with synthetic data generated both with standard oversampling techniques such as {SMOTE} and Autoencoders as well as with state of the art {GAN}-based models.},
	journaltitle = {{arXiv}:1901.02514 [cs, stat]},
	author = {Ger, Stephanie and Klabjan, Diego},
	urldate = {2021-01-12},
	date = {2020-08-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.02514},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ger and Klabjan - 2020 - Autoencoders and Generative Adversarial Networks f.pdf:C\:\\Users\\kendr\\Zotero\\storage\\GNABUV3Y\\Ger and Klabjan - 2020 - Autoencoders and Generative Adversarial Networks f.pdf:application/pdf},
}

@article{zhao_monocular_2020,
	title = {Monocular Depth Estimation Based On Deep Learning: An Overview},
	volume = {63},
	issn = {1674-7321, 1869-1900},
	url = {http://arxiv.org/abs/2003.06620},
	doi = {10.1007/s11431-020-1582-8},
	shorttitle = {Monocular Depth Estimation Based On Deep Learning},
	abstract = {Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semisupervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.},
	pages = {1612--1627},
	number = {9},
	journaltitle = {Sci. China Technol. Sci.},
	author = {Zhao, Chaoqiang and Sun, Qiyu and Zhang, Chongzhen and Tang, Yang and Qian, Feng},
	urldate = {2021-01-12},
	date = {2020-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.06620},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhao et al. - 2020 - Monocular Depth Estimation Based On Deep Learning.pdf:C\:\\Users\\kendr\\Zotero\\storage\\JKX7FXEM\\Zhao et al. - 2020 - Monocular Depth Estimation Based On Deep Learning.pdf:application/pdf},
}

@article{dao_wireless_2015,
	title = {Wireless laptop-based phonocardiograph and diagnosis},
	volume = {3},
	issn = {2167-8359},
	url = {https://peerj.com/articles/1178},
	doi = {10.7717/peerj.1178},
	abstract = {Auscultation is used to evaluate heart health, and can indicate when it’s needed to refer a patient to a cardiologist. Advanced phonocardiograph ({PCG}) signal processing algorithms are developed to assist the physician in the initial diagnosis but they are primarily designed and demonstrated with research quality equipment. Therefore, there is a need to demonstrate the applicability of those techniques with consumer grade instrument. Furthermore, routine monitoring would benefit from a wireless {PCG} sensor that allows continuous monitoring of cardiac signals of patients in physical activity, e.g., treadmill or weight exercise. In this work, a low-cost portable and wireless healthcare monitoring system based on {PCG} signal is implemented to validate and evaluate the most advanced algorithms. Off-the-shelf electronics and a notebook {PC} are used with {MATLAB} codes to record and analyze {PCG} signals which are collected with a notebook computer in tethered and wireless mode. Physiological parameters based on the S1 and S2 signals and {MATLAB} codes are demonstrated.},
	pages = {e1178},
	journaltitle = {{PeerJ}},
	author = {Dao, Amy T.},
	urldate = {2021-01-12},
	date = {2015-08-11},
	langid = {english},
	file = {Dao - 2015 - Wireless laptop-based phonocardiograph and diagnos.pdf:C\:\\Users\\kendr\\Zotero\\storage\\4Q5XM2FP\\Dao - 2015 - Wireless laptop-based phonocardiograph and diagnos.pdf:application/pdf},
}

@article{aziz_phonocardiogram_2020,
	title = {Phonocardiogram Signal Processing for Automatic Diagnosis of Congenital Heart Disorders through Fusion of Temporal and Cepstral Features},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/13/3790},
	doi = {10.3390/s20133790},
	abstract = {Congenital heart disease ({CHD}) is a heart disorder associated with the devastating indications that result in increased mortality, increased morbidity, increased healthcare expenditure, and decreased quality of life. Ventricular Septal Defects ({VSDs}) and Arterial Septal Defects ({ASDs}) are the most common types of {CHD}. {CHDs} can be controlled before reaching a serious phase with an early diagnosis. The phonocardiogram ({PCG}) or heart sound auscultation is a simple and non-invasive technique that may reveal obvious variations of different {CHDs}. Diagnosis based on heart sounds is difﬁcult and requires a high level of medical training and skills due to human hearing limitations and the non-stationary nature of {PCGs}. An automated computer-aided system may boost the diagnostic objectivity and consistency of {PCG} signals in the detection of {CHDs}. The objective of this research was to assess the effects of various pattern recognition modalities for the design of an automated system that effectively differentiates normal, {ASD}, and {VSD} categories using short term {PCG} time series. The proposed model in this study adopts three-stage processing: pre-processing, feature extraction, and classiﬁcation. Empirical mode decomposition ({EMD}) was used to denoise the raw {PCG} signals acquired from subjects. One-dimensional local ternary patterns (1D-{LTPs}) and Mel-frequency cepstral coefﬁcients ({MFCCs}) were extracted from the denoised {PCG} signal for precise representation of data from different classes. In the ﬁnal stage, the fused feature vector of 1D-{LTPs} and {MFCCs} was fed to the support vector machine ({SVM}) classiﬁer using 10-fold cross-validation. The {PCG} signals were acquired from the subjects admitted to local hospitals and classiﬁed by applying various experiments. The proposed methodology achieves a mean accuracy of 95.24\% in classifying {ASD}, {VSD}, and normal subjects. The proposed model can be put into practice and serve as a second opinion for cardiologists by providing more objective and faster interpretations of {PCG} signals.},
	pages = {3790},
	number = {13},
	journaltitle = {Sensors},
	author = {Aziz, Sumair and Khan, Muhammad Umar and Alhaisoni, Majed and Akram, Tallha and Altaf, Muhammad},
	urldate = {2021-01-12},
	date = {2020-07-06},
	langid = {english},
	file = {Aziz et al. - 2020 - Phonocardiogram Signal Processing for Automatic Di.pdf:C\:\\Users\\kendr\\Zotero\\storage\\7DPJ9HVT\\Aziz et al. - 2020 - Phonocardiogram Signal Processing for Automatic Di.pdf:application/pdf},
}

@inproceedings{bashar_heart_2018,
	location = {Sarawak, Malaysia},
	title = {Heart Abnormality Classification Using Phonocardiogram ({PCG}) Signals},
	isbn = {978-1-5386-2471-5},
	url = {https://ieeexplore.ieee.org/document/8626627/},
	doi = {10.1109/IECBES.2018.8626627},
	abstract = {Heart abnormality or disease is one of the leading causes of mortality worldwide. Sound signal produced by the mechanical activity of heart, known as phonocardiogram ({PCG}), provides useful information about the heart’s health. To increase discriminability among {PCG} signals of different normal and abnormal persons, an appropriate combination of signal features and classifiers is important. The segmentation of {PCG} signal, which requires corresponding {ECG} signal, is typically used for better prediction. But using {ECG} is generally expensive and time consuming. 781039In this paper, we therefore propose a segmentation free method to extract information from {PCG} signal. The signal is first preprocessed for {DC} removal and to limit the frequency to the required range. Four features (i.e. {WPS}, {PS}, {FD}, and {SF}) and four classifiers (i.e. {LDA}, {ESVM}, {DT}, and {KNN}) are then considered for the classification of heart murmur sound from {PCG} signals. A preliminary experiment with 56 signals showed the highest classification accuracy of 82.6\%, obtained by simple statistical feature ({SF}) with {ESVM} classifier. On average, the best performing classifier was {ESVM} (accuracy: 77.17\%), while the best feature was {PS} (accuracy: 75\%). In addition, the {PS} feature showed stable and consistent performance irrespective of the classifiers used. Results also indicate the importance of combining multiple features and classifiers for better accuracy and reliability.},
	eventtitle = {2018 {IEEE}-{EMBS} Conference on Biomedical Engineering and Sciences ({IECBES})},
	pages = {336--340},
	booktitle = {2018 {IEEE}-{EMBS} Conference on Biomedical Engineering and Sciences ({IECBES})},
	publisher = {{IEEE}},
	author = {Bashar, Md. Khayrul and Dandapat, Samarendra and Kumazawa, Itsuo},
	urldate = {2021-01-12},
	date = {2018-12},
	langid = {english},
	file = {Bashar et al. - 2018 - Heart Abnormality Classification Using Phonocardio.pdf:C\:\\Users\\kendr\\Zotero\\storage\\BKFV833W\\Bashar et al. - 2018 - Heart Abnormality Classification Using Phonocardio.pdf:application/pdf},
}

@inproceedings{potes_ensemble_2016,
	title = {Ensemble of Feature:based and Deep learning:based Classifiers for Detection of Abnormal Heart Sounds},
	url = {http://www.cinc.org/archives/2016/pdf/182-399.pdf},
	doi = {10.22489/CinC.2016.182-399},
	shorttitle = {Ensemble of Feature},
	abstract = {The goal of the 2016 {PhysioNet}/{CinC} Challenge is the development of an algorithm to classify normal/abnormal heart sounds. A total of 124 time-frequency features were extracted from the phonocardiogram ({PCG}) and input to a variant of the {AdaBoost} classiﬁer. A second classiﬁer using convolutional neural network ({CNN}) was trained using {PCGs} cardiac cycles decomposed into four frequency bands. The ﬁnal decision rule to classify normal/abnormal heart sounds was based on an ensemble of classiﬁers combining the outputs of {AdaBoost} and the {CNN}. The algorithm was trained on a training dataset (normal= 2575, abnormal= 665) and evaluated on a blind test dataset. Our classiﬁer ensemble approach obtained the highest score of the competition with a sensitivity, speciﬁcity, and overall score of 0.9424, 0.7781, and 0.8602, respectively.},
	eventtitle = {2016 Computing in Cardiology Conference},
	author = {Potes, Cristhian and Parvaneh, Saman and Rahman, Asif and Conroy, Bryan},
	urldate = {2021-01-12},
	date = {2016-09-14},
	langid = {english},
	file = {Potes et al. - 2016 - Ensemble of Featurebased and Deep learningbased .pdf:C\:\\Users\\kendr\\Zotero\\storage\\PSVHYRJF\\Potes et al. - 2016 - Ensemble of Featurebased and Deep learningbased .pdf:application/pdf},
}

@article{noauthor_s1_2017,
	title = {S1 and S2 Heart Sound Recognition Using Deep Neural Networks},
	volume = {64},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/7460939/},
	doi = {10.1109/TBME.2016.2559800},
	abstract = {Objective: This study focuses on the first (S1) and second (S2) heart sound recognition based only on acoustic characteristics; the assumptions of the individual durations of S1 and S2 and time intervals of S1–S2 and S2–S1 are not involved in the recognition process. The main objective is to investigate whether reliable S1 and S2 recognition performance can still be attained under situations where the duration and interval information might not be accessible. Methods: A deep neural network ({DNN}) method is proposed for recognizing S1 and S2 heart sounds. In the proposed method, heart sound signals are first converted into a sequence of Mel-frequency cepstral coefficients ({MFCCs}). The Kmeans algorithm is applied to cluster {MFCC} features into two groups to refine their representation and discriminative capability. The refined features are then fed to a {DNN} classifier to perform S1 and S2 recognition. We conducted experiments using actual heart sound signals recorded using an electronic stethoscope. Precision, recall, F-measure, and accuracy are used as the evaluation metrics. Results: The proposed {DNN}-based method can achieve high precision, recall, and F-measure scores with more than 91\% accuracy rate. Conclusion: The {DNN} classifier provides higher evaluation scores compared with other well-known pattern classification methods. Significance: The proposed {DNN}-based method can achieve reliable S1 and S2 recognition performance based on acoustic characteristics without using an {ECG} reference or incorporating the assumptions of the individual durations of S1 and S2 and time intervals of S1–S2 and S2–S1.},
	pages = {372--380},
	number = {2},
	journaltitle = {{IEEE} Trans. Biomed. Eng.},
	urldate = {2021-01-12},
	date = {2017-02},
	langid = {english},
	file = {2017 - S1 and S2 Heart Sound Recognition Using Deep Neura.pdf:C\:\\Users\\kendr\\Zotero\\storage\\NBYLILUG\\2017 - S1 and S2 Heart Sound Recognition Using Deep Neura.pdf:application/pdf},
}

@article{redlarski_system_2014,
	title = {A System for Heart Sounds Classification},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0112673},
	doi = {10.1371/journal.pone.0112673},
	abstract = {The future of quick and efficient disease diagnosis lays in the development of reliable non-invasive methods. As for the cardiac diseases – one of the major causes of death around the globe – a concept of an electronic stethoscope equipped with an automatic heart tone identification system appears to be the best solution. Thanks to the advancement in technology, the quality of phonocardiography signals is no longer an issue. However, appropriate algorithms for autodiagnosis systems of heart diseases that could be capable of distinguishing most of known pathological states have not been yet developed. The main issue is non-stationary character of phonocardiography signals as well as a wide range of distinguishable pathological heart sounds. In this paper a new heart sound classification technique, which might find use in medical diagnostic systems, is presented. It is shown that by combining Linear Predictive Coding coefficients, used for future extraction, with a classifier built upon combining Support Vector Machine and Modified Cuckoo Search algorithm, an improvement in performance of the diagnostic system, in terms of accuracy, complexity and range of distinguishable heart sounds, can be made. The developed system achieved accuracy above 93\% for all considered cases including simultaneous identification of twelve different heart sound classes. The respective system is compared with four different major classification methods, proving its reliability.},
	pages = {e112673},
	number = {11},
	journaltitle = {{PLoS} {ONE}},
	author = {Redlarski, Grzegorz and Gradolewski, Dawid and Palkowski, Aleksander},
	editor = {Talkachova, Alena},
	urldate = {2021-01-12},
	date = {2014-11-13},
	langid = {english},
	file = {Redlarski et al. - 2014 - A System for Heart Sounds Classification.pdf:C\:\\Users\\kendr\\Zotero\\storage\\9A7G8LB7\\Redlarski et al. - 2014 - A System for Heart Sounds Classification.pdf:application/pdf},
}

@article{liu_open_2016,
	title = {An open access database for the evaluation of heart sound algorithms},
	volume = {37},
	issn = {0967-3334, 1361-6579},
	url = {https://iopscience.iop.org/article/10.1088/0967-3334/37/12/2181},
	doi = {10.1088/0967-3334/37/12/2181},
	pages = {2181--2213},
	number = {12},
	journaltitle = {Physiol. Meas.},
	author = {Liu, Chengyu and Springer, David and Li, Qiao and Moody, Benjamin and Juan, Ricardo Abad and Chorro, Francisco J and Castells, Francisco and Roig, José Millet and Silva, Ikaro and Johnson, Alistair E W and Syed, Zeeshan and Schmidt, Samuel E and Papadaniil, Chrysa D and Hadjileontiadis, Leontios and Naseri, Hosein and Moukadem, Ali and Dieterlen, Alain and Brandt, Christian and Tang, Hong and Samieinasab, Maryam and Samieinasab, Mohammad Reza and Sameni, Reza and Mark, Roger G and Clifford, Gari D},
	urldate = {2021-01-12},
	date = {2016-12-01},
	langid = {english},
	file = {Liu et al. - 2016 - An open access database for the evaluation of hear.pdf:C\:\\Users\\kendr\\Zotero\\storage\\SKYBTCIW\\Liu et al. - 2016 - An open access database for the evaluation of hear.pdf:application/pdf},
}

@article{jia_direct_2019,
	title = {Direct speech-to-speech translation with a sequence-to-sequence model},
	url = {http://arxiv.org/abs/1904.06037},
	abstract = {We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and ﬁnd that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.},
	journaltitle = {{arXiv}:1904.06037 [cs, eess]},
	author = {Jia, Ye and Weiss, Ron J. and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
	urldate = {2021-01-13},
	date = {2019-06-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.06037},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Jia et al. - 2019 - Direct speech-to-speech translation with a sequenc.pdf:C\:\\Users\\kendr\\Zotero\\storage\\GYUGQP8P\\Jia et al. - 2019 - Direct speech-to-speech translation with a sequenc.pdf:application/pdf},
}

@article{ali_denoising_2017,
	title = {Denoising of Heart Sound Signals Using Discrete Wavelet Transform},
	volume = {36},
	issn = {0278-081X, 1531-5878},
	url = {http://link.springer.com/10.1007/s00034-017-0524-7},
	doi = {10.1007/s00034-017-0524-7},
	abstract = {Signal denoising remains to be one of the main problems in the ﬁeld of signal processing. Various signal denoising algorithms using wavelet transforms have been introduced. Wavelets show superior signal denoising performance due to their properties such as multiresolution and windowing. This study focuses on denoising of phonocardiogram ({PCG}) signals using different families of discrete wavelet transforms, thresholding types and techniques, and signal decomposition levels. In particular, we discuss the effect of the chosen wavelet function and wavelet decomposition level on the efﬁciency of the denoising algorithm. Denoised signals are compared with the original {PCG} signal to determine the most suitable parameters (wavelet family, level of decomposition, and thresholding type) for the denoising process. The performance of our algorithm is evaluated using the signal-to-noise ratio, percentage root-mean-square difference, and root-mean-square error. The results show that the level of decomposition and thresholding type are the most important parameters affecting the efﬁciency of the denoising algorithm. Finally, we compare our results with those from other studies to test and optimize the performance of the proposed algorithm.},
	pages = {4482--4497},
	number = {11},
	journaltitle = {Circuits Syst Signal Process},
	author = {Ali, Mohammed Nabih and El-Dahshan, {EL}-Sayed A. and Yahia, Ashraf H.},
	urldate = {2021-01-20},
	date = {2017-11},
	langid = {english},
	file = {Ali et al. - 2017 - Denoising of Heart Sound Signals Using Discrete Wa.pdf:C\:\\Users\\kendr\\Zotero\\storage\\48WGA8RD\\Ali et al. - 2017 - Denoising of Heart Sound Signals Using Discrete Wa.pdf:application/pdf},
}

@inproceedings{babu_s1_2017,
	location = {Penang},
	title = {S1 and S2 heart sound segmentation using variational mode decomposition},
	isbn = {978-1-5090-1134-6},
	url = {http://ieeexplore.ieee.org/document/8228119/},
	doi = {10.1109/TENCON.2017.8228119},
	abstract = {Heart sound segmentation is the process of identifying the various events like S1, S2, S3, S4, and murmurs present in the phonocardiogram ({PCG}) signal. In this paper, a heart sound segmentation algorithm for segmenting S1 and S2 heart sound which corresponds to the systolic and diastolic activity of the heart is presented. The proposed method is based on variational mode decomposition ({VMD}) algorithm. The recorded {PCG} is decomposed into various modes using {VMD}. A novel criterion is proposed to select an appropriate mode which is based on center frequency and relative energy. Then the Shannon entropy envelope of the selected mode is computed. S1 and S2 heart sound is segmented by applying dynamic threshold on the Shannon entropy envelope. The segmentation algorithm is tested on both available standard database (Physionet, Pascal, Michigan, and {eGeneralMedical}) and recorded signal. The simulation results demonstrate that the proposed method achieved the sensitivity, positive predictivity, and the accuracy of above 95\% with detection error rate of below 6\%.},
	eventtitle = {{TENCON} 2017 - 2017 {IEEE} Region 10 Conference},
	pages = {1629--1634},
	booktitle = {{TENCON} 2017 - 2017 {IEEE} Region 10 Conference},
	publisher = {{IEEE}},
	author = {Babu, K. Ajay and Ramkumar, Barathram and Manikandan, M. Sabarimalai},
	urldate = {2021-01-20},
	date = {2017-11},
	langid = {english},
	file = {Babu et al. - 2017 - S1 and S2 heart sound segmentation using variation.pdf:C\:\\Users\\kendr\\Zotero\\storage\\TEVTGAA3\\Babu et al. - 2017 - S1 and S2 heart sound segmentation using variation.pdf:application/pdf},
}

@article{zhang_heart_2017,
	title = {Heart sound classification based on scaled spectrogram and partial least squares regression},
	volume = {32},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809416301616},
	doi = {10.1016/j.bspc.2016.10.004},
	pages = {20--28},
	journaltitle = {Biomedical Signal Processing and Control},
	author = {Zhang, Wenjie and Han, Jiqing and Deng, Shiwen},
	urldate = {2021-01-25},
	date = {2017-02},
	langid = {english},
	file = {Zhang et al. - 2017 - Heart sound classification based on scaled spectro.pdf:C\:\\Users\\kendr\\Zotero\\storage\\JB9ZACBG\\Zhang et al. - 2017 - Heart sound classification based on scaled spectro.pdf:application/pdf},
}

@article{abo-zahhad_comparative_2016,
	title = {A comparative approach between cepstral features for human authentication using heart sounds},
	volume = {10},
	issn = {1863-1703, 1863-1711},
	url = {http://link.springer.com/10.1007/s11760-015-0826-9},
	doi = {10.1007/s11760-015-0826-9},
	abstract = {The main objective of this paper is to provide a comparative study between different cepstral features for the application of human recognition using heart sounds. In the past 10 years, heart sound, which is known as phonocardiogram, has been adopted for human biometric authentication tasks. Most of the previously proposed systems have adopted mel-frequency and linear frequency cepstral coefﬁcients as features for heart sounds. In this paper, two more cepstral features are proposed. The ﬁrst one is based on wavelet packet decomposition where a new ﬁlter bank structure is designed to select the appropriate bases for extracting discriminant features from heart sounds. The other is based on nonlinear modiﬁcation for mel-scaled cepstral features. The four cepstral features are tested and compared on two databases: One consists of 21 subjects, and the other consists of 206 subjects. Based on the achieved results over the two databases, the two proposed cepstral features achieved higher correct recognition rates and lower error rates in identiﬁcation and veriﬁcation modes, respectively.},
	pages = {843--851},
	number = {5},
	journaltitle = {{SIViP}},
	author = {Abo-Zahhad, M. and Farrag, Mohammed and Abbas, Sherif N. and Ahmed, Sabah M.},
	urldate = {2021-01-25},
	date = {2016-07},
	langid = {english},
	file = {Abo-Zahhad et al. - 2016 - A comparative approach between cepstral features f.pdf:C\:\\Users\\kendr\\Zotero\\storage\\5CDFL755\\Abo-Zahhad et al. - 2016 - A comparative approach between cepstral features f.pdf:application/pdf},
}

@article{messner_heart_2018,
	title = {Heart Sound Segmentation—An Event Detection Approach Using Deep Recurrent Neural Networks},
	volume = {65},
	issn = {0018-9294, 1558-2531},
	url = {https://ieeexplore.ieee.org/document/8370727/},
	doi = {10.1109/TBME.2018.2843258},
	abstract = {Objective: In this paper, we accurately detect the state-sequence ﬁrst heart sound (S1) - systole second heart sound (S2) - diastole, i.e. the positions of S1 and S2, in heart sound recordings. We propose an event detection approach, without explicitly incorporating a priori information of the state duration. This renders it also applicable to recordings with cardiac arrhythmia and extendable to the detection of extra heart sounds (third and fourth heart sound), heart murmurs, as well as other acoustic events. Methods: We use data from the 2016 {PhysioNet}/{CinC} Challenge, containing heart sound recordings and annotations of the heart sound states. From the recordings, we extract spectral and envelope features and investigate the performance of different deep recurrent neural network ({DRNN}) architectures to detect the state-sequence. We use virtual-adversarial training ({VAT}), dropout and data augmentation for regularization. Results: We compare our results with the state-of-the-art method and achieve an average score for the four events of the state-sequence of F1 ≈ 96\% on an independent test set. Conclusion: Our approach shows state-of-the-art performance carefully evaluated on the 2016 {PhysioNet}/{CinC} Challenge dataset. Signiﬁcance: In this work, we introduce a new methodology for the segmentation of heart sounds, suggesting an event detection approach with {DRNNs} using spectral or envelope features.},
	pages = {1964--1974},
	number = {9},
	journaltitle = {{IEEE} Trans. Biomed. Eng.},
	author = {Messner, Elmar and Zohrer, Matthias and Pernkopf, Franz},
	urldate = {2021-01-25},
	date = {2018-09},
	langid = {english},
	file = {Messner et al. - 2018 - Heart Sound Segmentation—An Event Detection Approa.pdf:C\:\\Users\\kendr\\Zotero\\storage\\EY4IP9Q2\\Messner et al. - 2018 - Heart Sound Segmentation—An Event Detection Approa.pdf:application/pdf},
}

@article{esser_taming_2021,
	title = {Taming Transformers for High-Resolution Image Synthesis},
	url = {http://arxiv.org/abs/2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to {CNNs}, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of {CNNs} with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use {CNNs} to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://compvis.github.io/taming-transformers/ .},
	journaltitle = {{arXiv}:2012.09841 [cs]},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	urldate = {2021-02-18},
	date = {2021-02-11},
	eprinttype = {arxiv},
	eprint = {2012.09841},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf:C\:\\Users\\kendr\\Zotero\\storage\\H3TMZS9J\\Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\XPFZ596E\\2012.html:text/html;Full Text:C\:\\Users\\kendr\\Zotero\\storage\\W7GKZ9NA\\Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf:application/pdf},
}

@online{noauthor_photo_nodate,
	title = {Photo Posters - Create Custom Photo Posters {\textbar} Walgreens Photo},
	url = {https://photo.walgreens.com/store/poster-details?tab=photo_Posters-Posters#!/pdpview},
	urldate = {2021-02-18},
	file = {Photo Posters - Create Custom Photo Posters | Walgreens Photo:C\:\\Users\\kendr\\Zotero\\storage\\WEDTD3JV\\poster-details.html:text/html},
}

@article{jia_direct_2019-1,
	title = {Direct speech-to-speech translation with a sequence-to-sequence model},
	url = {http://arxiv.org/abs/1904.06037},
	abstract = {We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.},
	journaltitle = {{arXiv}:1904.06037 [cs, eess]},
	author = {Jia, Ye and Weiss, Ron J. and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
	urldate = {2021-02-19},
	date = {2019-06-25},
	eprinttype = {arxiv},
	eprint = {1904.06037},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Jia et al_2019_Direct speech-to-speech translation with a sequence-to-sequence model.pdf:C\:\\Users\\kendr\\Zotero\\storage\\Q9T7N7HG\\Jia et al_2019_Direct speech-to-speech translation with a sequence-to-sequence model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kendr\\Zotero\\storage\\WKQRHBVL\\1904.html:text/html},
}